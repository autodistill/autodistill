{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Autodistill uses big, slower foundation models to train small, faster supervised models. Using <code>autodistill</code>, you can go from unlabeled images to inference on a custom model running at the edge with no human intervention in between.</p> <p>You can use Autodistill on your own hardware, or use the Roboflow hosted version of Autodistill to label images in the cloud.</p> <p> </p> <p>Here are example predictions of a Target Model detecting milk bottles and bottlecaps after being trained on an auto-labeled dataset using Autodistill (see the Autodistill YouTube video for a full walkthrough):</p> <p> </p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>\ud83d\udd0c Pluggable interface to connect models together</li> <li>\ud83e\udd16 Automatically label datasets</li> <li>\ud83d\udc30 Train fast supervised models</li> <li>\ud83d\udd12 Own your model</li> <li>\ud83d\ude80 Deploy distilled models to the cloud or the edge</li> </ul>"},{"location":"#basic-concepts","title":"\ud83d\udcda Basic Concepts","text":"<p>To use <code>autodistill</code>, you input unlabeled data into a Base Model which uses an Ontology to label a Dataset that is used to train a Target Model which outputs a Distilled Model fine-tuned to perform a specific Task.</p> <p> </p> <p>Autodistill defines several basic primitives:</p> <ul> <li>Task - A Task defines what a Target Model will predict. The Task for each component (Base Model, Ontology, and Target Model) of an <code>autodistill</code> pipeline must match for them to be compatible with each other. Object Detection and Instance Segmentation are currently supported through the <code>detection</code> task. <code>classification</code> support will be added soon.</li> <li>Base Model - A Base Model is a large foundation model that knows a lot about a lot. Base models are often multimodal and can perform many tasks. They're large, slow, and expensive. Examples of Base Models are GroundedSAM and GPT-4's upcoming multimodal variant. We use a Base Model (along with unlabeled input data and an Ontology) to create a Dataset.</li> <li>Ontology - an Ontology defines how your Base Model is prompted, what your Dataset will describe, and what your Target Model will predict. A simple Ontology is the <code>CaptionOntology</code> which prompts a Base Model with text captions and maps them to class names. Other Ontologies may, for instance, use a CLIP vector or example images instead of a text caption.</li> <li>Dataset - a Dataset is a set of auto-labeled data that can be used to train a Target Model. It is the output generated by a Base Model.</li> <li>Target Model - a Target Model is a supervised model that consumes a Dataset and outputs a distilled model that is ready for deployment. Target Models are usually small, fast, and fine-tuned to perform a specific task very well (but they don't generalize well beyond the information described in their Dataset). Examples of Target Models are YOLOv8 and DETR.</li> <li>Distilled Model - a Distilled Model is the final output of the <code>autodistill</code> process; it's a set of weights fine-tuned for your task that can be deployed to get predictions.</li> </ul>"},{"location":"#theory-and-limitations","title":"\ud83d\udca1 Theory and Limitations","text":"<p>Human labeling is one of the biggest barriers to broad adoption of computer vision. It can take thousands of hours to craft a dataset suitable for training a production model. The process of distillation for training supervised models is not new, in fact, traditional human labeling is just another form of distillation from an extremely capable Base Model (the human brain \ud83e\udde0).</p> <p>Foundation models know a lot about a lot, but for production we need models that know a lot about a little.</p> <p>As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process. We need tools for steering, utilizing, and comparing these models. Additionally, these foundation models are big, expensive, and often gated behind private APIs. For many production use-cases, we need models that can run cheaply and in realtime at the edge.</p> <p> </p> <p>Autodistill's Base Models can already create datasets for many common use-cases (and through creative prompting and few-shotting we can expand their utility to many more), but they're not perfect yet. There's still a lot of work to do; this is just the beginning and we'd love your help testing and expanding the capabilities of the system!</p>"},{"location":"#installation","title":"\ud83d\udcbf Installation","text":"<p>Autodistill is modular. You'll need to install the <code>autodistill</code> package (which defines the interfaces for the above concepts) along with Base Model and Target Model plugins (which implement specific models).</p> <p>By packaging these separately as plugins, dependency and licensing incompatibilities are minimized and new models can be implemented and maintained by anyone.</p> <p>Example:  <pre><code>pip install autodistill autodistill-grounded-sam autodistill-yolov8\n</code></pre></p> Install from source  You can also clone the project from GitHub for local development:  <pre><code>git clone https://github.com/roboflow/autodistill\ncd autodistill\npip install -e .\n</code></pre> <p>Additional Base and Target models are enumerated below.</p>"},{"location":"#video-guides","title":"\ud83c\udfac Video Guides","text":"<p> Autodistill: Train YOLOv8 with ZERO Annotations Published: 8 June 2023 In this video, we will show you how to use a new library to train a YOLOv8 model to detect bottles moving on a conveyor line. Yes, that's right - zero annotation hours are required! We dive deep into Autodistill's functionality, covering topics from setting up your Python environment and preparing your images, to the thrilling automatic annotation of images. </p>"},{"location":"#community-resources","title":"\ud83d\udca1 Community Resources","text":"<ul> <li>Distill Large Vision Models into Smaller, Efficient Models with Autodistill: Announcement post with written guide on how to use Autodistill</li> <li>Comparing AI-Labeled Data to Human-Labeled Data: A qualitative evaluation of Grounding DINO used with Autodistill across various tasks and domains.</li> <li>How to Evaluate Autodistill Prompts with CVevals: Evaluate Autodistill prompts.</li> <li>Autodistill: Label and Train a Computer Vision Model in Under 20 Minutes: Building a model to detect planes in under 20 minutes.</li> <li>Comparing AI-Labeled Data to Human-Labeled Data: Explore the strengths and limitations of a base model used with Autoditsill.</li> <li>Train an Image Classification Model with No Labeling: Use Grounded SAM to automatically label images for training an Ultralytics YOLOv8 classification model.</li> <li>Train a Segmentation Model with No Labeling: Use CLIP to automatically label images for training an Ultralytics YOLOv8 segmentation model.</li> <li>File a PR to add your own resources here!</li> </ul>"},{"location":"#roadmap","title":"\ud83d\uddfa\ufe0f Roadmap","text":"<p>Apart from adding new models, there are several areas we plan to explore with <code>autodistill</code> including:</p> <ul> <li>\ud83d\udca1 Ontology creation &amp; prompt engineering</li> <li>\ud83d\udc69\u200d\ud83d\udcbb Human in the loop support</li> <li>\ud83e\udd14 Model evaluation</li> <li>\ud83d\udd04 Active learning</li> <li>\ud83d\udcac Language tasks</li> </ul>"},{"location":"#contributing","title":"\ud83c\udfc6 Contributing","text":"<p>We love your input! Please see our contributing guide to get started. Thank you \ud83d\ude4f to all our contributors!</p>"},{"location":"#license","title":"\ud83d\udc69\u200d\u2696\ufe0f License","text":"<p>The <code>autodistill</code> package is licensed under an Apache 2.0. Each Base or Target model plugin may use its own license corresponding with the license of its underlying model. Please refer to the license in each plugin repo for more information.</p>"},{"location":"#frequently-asked-questions","title":"Frequently Asked Questions \u2753","text":""},{"location":"#what-causes-the-pytorchstreamreader-failed-reading-zip-archive-failed-finding-central-directory-error","title":"What causes the <code>PytorchStreamReader failed reading zip archive: failed finding central directory</code> error?","text":"<p>This error is caused when PyTorch cannot load the model weights for a model. Go into the <code>~/.cache/autodistill</code> directory and delete the folder associated with the model you are trying to load. Then, run your code again. The model weights will be downloaded from scratch. Leave the installation process uninterrupted.</p>"},{"location":"#explore-more-roboflow-open-source-projects","title":"\ud83d\udcbb explore more Roboflow open source projects","text":"Project Description supervision General-purpose utilities for use in computer vision projects, from predictions filtering and display to object tracking to model evaluation. Autodistill (this project) Automatically label images for use in training computer vision models. Inference An easy-to-use, production-ready inference server for computer vision supporting deployment of many popular model architectures and fine-tuned models. Notebooks Tutorials for computer vision tasks, from training state-of-the-art models to tracking objects to counting objects in a zone. Collect Automated, intelligent data collection powered by CLIP."},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"changelog/#010-2023-05-17","title":"[0.1.0] - 2023-05-17","text":"<p>Launched the <code>autodistill</code> package with support for GroundingSAM.</p>"},{"location":"command-line-interface/","title":"Command Line Interface","text":"<p>You can use Autodistill with a command line interface (CLI). </p> <p>The CLI allows you to run inference on a model or auto-label a folder of imageswihout writing a labeling script.</p>"},{"location":"command-line-interface/#installation","title":"Installation","text":"<p>To install the CLI, install the Autodistill Python package:</p> <pre><code>pip install autodistill\n</code></pre> <p>The CLI accepts several arguments:</p> <ul> <li><code>images</code>: The path to the folder of images you want to label.</li> <li><code>--base</code>: The base model you want to use for labeling. This can be any model from the Autodistill Model Zoo.</li> <li><code>--target</code>: The target model you want to use to train a model with your labeled dataset.</li> <li><code>--ontology</code>: The ontology you want to use for labeling. This must be a mapping of text prompts to send to a model to the label you want to save in your dataset. For example, <code>{\"acoustic guitar\": \"guitar\"}</code> will send the text prompt <code>acoustic guitar</code> to the model, then save the label as <code>guitar</code> in your dataset.</li> <li><code>--output</code>: The path to the folder where you want to save your labeled dataset.</li> </ul> <p>Here is an example:</p> <pre><code>autodistill images --base=\"grounding_dino\" --target=\"yolov8\" --ontology '{\"prompt\": \"label\"}' --output=\"./dataset\"\n</code></pre> <p>This command will label all images in a directory called <code>images</code> with Grounding DINO and use the labeled images to train a YOLOv8 model. Grounding DINO will label all images with the \"prompt\" and save the label as the \"label\". You can specify as many prompts and labels as you want. The resulting dataset will be saved in a folder called <code>dataset</code>.</p>"},{"location":"contributing/","title":"Contributing to Autodistill \ud83d\udee0\ufe0f","text":"<p>Thank you for your interest in contributing to Autodistill!</p> <p>We welcome any contributions to help us improve the quality of <code>autodistill</code> and expand the range of supported models.</p>"},{"location":"contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>We welcome contributions to:</p> <ol> <li>Add a new base model (see more guidance below).</li> <li>Add a new target model (see more guidance below).</li> <li>Report bugs and issues in the project.</li> <li>Submit a request for a new task or feature.</li> <li>Improve our test coverage.</li> </ol>"},{"location":"contributing/#contributing-features","title":"Contributing Features","text":"<p>Autodistill is designed with modularity in mind. We want <code>autodistill</code> to extend across different models and problem types, providing a consistent interface for distilling models.</p> <p>We welcome contributions that add new models to the project. Before you begin, please make sure that another contributor has not already begun work on the model you want to add. You can check the project README for our roadmap on adding more models.</p> <p>To add a new model, create a new repo that requires <code>autodistill</code> and implement the <code>BaseModel</code> or <code>TargetModel</code> class for your task. You can use the existing models as a guide for how to structure your code.</p> <p>Finally, you will need to add documentation for your model and link to it from the <code>autodistill</code> README. You can add a new page to the <code>docs/models</code> directory that describes your model and how to use it. You can use the existing model documentation as a guide for how to structure your documentation.</p>"},{"location":"contributing/#how-to-contribute-changes","title":"How to Contribute Changes","text":"<p>First, fork this repository to your own GitHub account. Create a new branch that describes your changes (i.e. <code>line-counter-docs</code>). Push your changes to the branch on your fork and then submit a pull request to this repository.</p> <p>When creating new functions, please ensure you have the following:</p> <ol> <li>Docstrings for the function and all parameters.</li> <li>Examples in the documentation for the function.</li> <li>Created an entry in our docs to autogenerate the documentation for the function.</li> </ol> <p>All pull requests will be reviewed by the maintainers of the project. We will provide feedback and ask for changes if necessary.</p> <p>PRs must pass all tests and linting requirements before they can be merged.</p>"},{"location":"contributing/#code-quality","title":"\ud83e\uddf9 Code quality","text":"<p>We provide two handy commands inside the <code>Makefile</code>, namely:</p> <ul> <li><code>make style</code> to format the code</li> <li><code>make check_code_quality</code> to check code quality (PEP8 basically)</li> </ul> <p>So far, there is no types checking with mypy. See issue. </p>"},{"location":"contributing/#tests","title":"\ud83e\uddea Tests","text":"<p><code>pytests</code> is used to run our tests.</p>"},{"location":"image-loading/","title":"Image Loading","text":"<p>All Autodistill base models (i.e. Grounding DINO or CLIP) support providing a file name and loading the corresponding image for use in labeling. Some models also enable passing images directly from the following formats:</p> <ul> <li>PIL <code>Image</code></li> <li>cv2 image</li> <li>URL, from which an image is retrieved</li> <li>A file name, which is loaded as an image</li> </ul> <p>This is handled by the low-level <code>load_image</code> function. This function allows you to pass any of the above formats. The PIL and cv2 formats are ideal if you already have an image in memory. Base models use this function to request the format the model needs. If a model needs an image in a format different from what you have provided -- for example, if you provided a file name and the model needs a PIL <code>Image</code> object -- the <code>load_image</code> function will convert the image to the correct format.</p> <p>The following models support the <code>load_image</code> function. The <code>PIL</code> and <code>cv2</code> states to what format <code>load_image</code> will convert your image (if necessary) to pass your image into a model.</p> <ul> <li>AltCLIP: PIL</li> <li>CLIP: PIL</li> <li>Grounding DINO: cv2</li> <li>MetaCLIP: PIL</li> <li>RemoteCLIP: PIL</li> <li>Transformers: PIL</li> <li>SAM HQ: cv2</li> <li>Segment Anything: cv2</li> <li>DETIC: PIL</li> <li>VLPart: PIL</li> <li>CoDet: PIL</li> <li>OWLv2: PIL</li> <li>FastViT: PIL</li> <li>FastSAM: cv2</li> <li>SegGPT: PIL</li> <li>OWLViT: PIL</li> <li>BLIPv2: PIL</li> <li>DINOv2: PIL</li> <li>Grounded SAM: cv2</li> <li>BLIP: PIL</li> </ul>"},{"location":"image-loading/#load_image-function","title":"<code>load_image</code> function","text":"<p>Load an image from a file path, URI, PIL image, or numpy array.</p> <p>This function is for use by Autodistill modules. You don't need to use it directly.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to load</p> required <code>return_format</code> <p>The format to return the image in</p> <code>'cv2'</code> <p>Returns:</p> Type Description <code>Any</code> <p>The image in the specified format</p> Source code in <code>autodistill/helpers.py</code> <pre><code>def load_image(\n    image: Any,\n    return_format=\"cv2\",\n) -&gt; Any:\n\"\"\"\n    Load an image from a file path, URI, PIL image, or numpy array.\n\n    This function is for use by Autodistill modules. You don't need to use it directly.\n\n    Args:\n        image: The image to load\n        return_format: The format to return the image in\n\n    Returns:\n        The image in the specified format\n    \"\"\"\n    if return_format not in ACCEPTED_RETURN_FORMATS:\n        raise ValueError(f\"return_format must be one of {ACCEPTED_RETURN_FORMATS}\")\n\n    if isinstance(image, Image.Image) and return_format == \"PIL\":\n        return image\n    elif isinstance(image, Image.Image) and return_format == \"cv2\":\n        # channels need to be reversed for cv2\n        return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n    elif isinstance(image, Image.Image) and return_format == \"numpy\":\n        return np.array(image)\n\n    if isinstance(image, np.ndarray) and return_format == \"PIL\":\n        return Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    elif isinstance(image, np.ndarray) and return_format == \"cv2\":\n        return image\n    elif isinstance(image, np.ndarray) and return_format == \"numpy\":\n        return image\n\n    if isinstance(image, str) and image.startswith(\"http\"):\n        if return_format == \"PIL\":\n            response = requests.get(image)\n            return Image.open(BytesIO(response.content))\n        elif return_format == \"cv2\" or return_format == \"numpy\":\n            response = requests.get(image)\n            pil_image = Image.open(BytesIO(response.content))\n            return np.array(pil_image)\n    elif os.path.isfile(image):\n        if return_format == \"PIL\":\n            return Image.open(image)\n        elif return_format == \"cv2\":\n            # channels need to be reversed for cv2\n            return cv2.cvtColor(np.array(Image.open(image)), cv2.COLOR_RGB2BGR)\n        elif return_format == \"numpy\":\n            pil_image = Image.open(image)\n            return np.array(pil_image)\n    else:\n        raise ValueError(f\"{image} is not a valid file path or URI\")\n</code></pre>"},{"location":"large-datasets/","title":"Labeling Large Datasets","text":"<p>Autodistill has not been optimized for labeling large datasets, but this work is in progress. In the mean time, we recommend labeling only a few hundred images at a time, to the maximum of how many images you can store in memory.</p>"},{"location":"large-datasets/#how-the-autodistill-labeling-process-works","title":"How the Autodistill labeling process works","text":"<p>During image labeling, a data structure is built that contains:</p> <ol> <li>A numpy representation of an image;</li> <li>The labels for the image, and;</li> <li>The image file name.</li> </ol> <p>If you are labeling large datasets, this data structure will get large. For example, if you have 10,000 images in a folder to label, this data structure will contain 10,000 images. This can cause memory issues if your system doesn't have enough memory to store all images.</p> <p>We are working on a system that will prevent the need to store images in memory during the labeling process. This system will also include an intelligent label resumption system, so if labeling stops for any reason you will be able to resume labeling from where you stopped.</p> <p>Follow Issue #93 in the Autodistill GitHub repository.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Autodistill lets you use large, foundation vision models to auto-label data for and train small, fine-tuned vision models. This process is called distillation.</p> <p>Your fine-tuned model will run smaller and faster, and will thus be more suitable for deployment on edge devices.</p>"},{"location":"quickstart/#how-autodistill-works","title":"How Autodistill Works","text":"<p>There are two main concepts in Autodistill:</p> <ul> <li>A base model, which is used to auto-label data. Examples include Grounding DINO, Grounded SAM, and CLIP.</li> <li>A target model, which is trained on the auto-labeled data. Examples include YOLOv5, YOLOv8, and DETR.</li> </ul> <p>You can use Autodistill with only a base model if you want to label data and run your own training.</p> <p>You can also use Autodistill with both a base model and a target model to build an end-to-end labeling and training pipeline.</p>"},{"location":"quickstart/#distill-a-model-tutorial","title":"Distill a Model (Tutorial)","text":"<p>Tip</p> <p>See the demo Notebook for a quick introduction to <code>autodistill</code>. This notebook walks through building a milk container detection model with no labeling.</p> <p>If you want to skip directly to the full code, without the tutorial, go to the Code Summary section.</p> <p>Let's distill a model to see how Autodistill works. We will use Autodistill to auto-label a milk bottle dataset.</p> <p>For this example, we'll show how to distill GroundedSAM into a small YOLOv8 model using autodistill-grounded-sam and autodistill-yolov8.</p>"},{"location":"quickstart/#step-1-install-autodistill-and-models","title":"Step #1: Install Autodistill and Models","text":"<p>First, install the required dependencies:</p> <pre><code>pip install autodistill autodistill-grounded-sam autodistill-yolov8\n</code></pre> <p>Tip</p> <p>See the Autodistill Supported Models list for a list of all supported models.</p>"},{"location":"quickstart/#step-2-set-an-ontology","title":"Step #2: Set an Ontology","text":"<p>Every base model needs an ontology. An ontology tells Autodistill what you want to identify and what labels should be called in your dataset.</p> <p>For example, if you want to identify milk bottles, you could use the following ontology:</p> <pre><code>{\n    \"milk bottle\": \"bottle\",\n    \"milk bottle cap\": \"bottle cap\"\n}\n</code></pre> <p>This ontology will tell Autodistill to identify milk bottles and milk bottle caps, and to save the labels as <code>bottle</code> and <code>bottle cap</code> in your dataset.</p>"},{"location":"quickstart/#step-3-set-up-the-model","title":"Step #3: Set up the Model","text":"<p>Let's set up our model. Create a new Python file and add the following lines of code:</p> <pre><code>from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_yolov8 import YOLOv8\nfrom autodistill.utils import plot\nimport cv2\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\nbase_model = GroundedSAM(ontology=CaptionOntology({\"milk bottle\": \"bottle\", \"milk bottle cap\": \"bottle cap\"}))\n</code></pre>"},{"location":"quickstart/#step-4-test-the-base-model","title":"Step #4: Test the Base Model","text":"<p>We can test our base model using the <code>predict</code> function:</p> <pre><code>results = base_model.predict(\"milk.jpg\")\n\nplot(\n    image=cv2.imread(\"milk.jpg\"),\n    classes=base_model.ontology.classes(),\n    detections=results\n)\n</code></pre>"},{"location":"quickstart/#step-5-label-a-dataset","title":"Step #5: Label a Dataset","text":"<p>Now that we have a base model, we can use it to label a dataset. You can label a dataset using the following code:</p> <pre><code>base_model.label_folder(\n    input_folder=\"./images\",\n    output_folder=\"./labeled-images\"\n)\n</code></pre>"},{"location":"quickstart/#step-6-train-a-target-model","title":"Step #6: Train a Target Model","text":"<p>We can use a target model like YOLOv8 to train a model on our labeled dataset. You can train a target model using the following code:</p> <pre><code>target_model = YOLOv8(\"yolov8n.pt\")\ntarget_model.train(\"./labeled-images/data.yaml\", epochs=200)\n</code></pre> <p>Your model weights will be saved in a folder called <code>runs</code>.</p> <p>For YOLOv8 models, you can then run inference locally using the ultralytics Python package, or deploy your model to Roboflow.</p>"},{"location":"quickstart/#code-summary","title":"Code Summary","text":"<p>Here is all of the code we used above, summarized into a single code snippet:</p> <pre><code>from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_yolov8 import YOLOv8\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n\nbase_model = GroundedSAM(ontology=CaptionOntology({\"milk bottle\": \"bottle\", \"milk bottle cap\": \"bottle cap\"}))\n\nresults = base_model.predict(\"milk.jpg\")\n\nbase_model.label_folder(\n    input_folder=\"./images\",\n    output_folder=\"./labeled-images\"\n)\n\ntarget_model = YOLOv8(\"yolov8n.pt\")\ntarget_model.train(\"./labeled-images/data.yaml\", epochs=200)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Above, we used Autodistill to label a dataset. Next, explore the Autodistill ecosystem of models following our guidance in the What model should I use? guide. This site contains documentation for all Autodistill models, as well as utilities that you can use to work with each model.</p>"},{"location":"supported-models/","title":"Supported Models","text":"<p>Our goal is for <code>autodistill</code> to support using all foundation models as Base Models and most SOTA supervised models as Target Models. We focused on object detection and segmentation tasks first but plan to launch classification support soon! In the future, we hope <code>autodistill</code> will also be used for models beyond computer vision.</p> <ul> <li>\u2705 - complete (click row/column header to go to repo)</li> <li>\ud83d\udea7 - work in progress</li> </ul>"},{"location":"supported-models/#object-detection","title":"object detection","text":"base / target YOLOv8 YOLO-NAS YOLOv5 DETR YOLOv6 YOLOv7 MT-YOLOv6 DETIC \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 GroundedSAM \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 GroundingDINO \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 OWL-ViT \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 SAM-CLIP \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 LLaVA-1.5 \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Kosmos-2 \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 OWLv2 \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Roboflow Universe Models (50k+ pre-trained models) \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 CoDet \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 VLPart \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Azure Custom Vision \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 AWS Rekognition \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Google Vision \u2705 \u2705 \u2705 \u2705 \ud83d\udea7"},{"location":"supported-models/#instance-segmentation","title":"instance segmentation","text":"base / target YOLOv8 YOLO-NAS YOLOv5 YOLOv7 Segformer GroundedSAM \u2705 \ud83d\udea7 \ud83d\udea7 SAM-CLIP \u2705 \ud83d\udea7 \ud83d\udea7 SegGPT \u2705 \ud83d\udea7 \ud83d\udea7 FastSAM \ud83d\udea7 \ud83d\udea7 \ud83d\udea7"},{"location":"supported-models/#classification","title":"classification","text":"base / target ViT YOLOv8 YOLOv5 CLIP \u2705 \u2705 \ud83d\udea7 MetaCLIP \u2705 \u2705 \ud83d\udea7 DINOv2 \u2705 \u2705 \ud83d\udea7 BLIP \u2705 \u2705 \ud83d\udea7 ALBEF \u2705 \u2705 \ud83d\udea7 FastViT \u2705 \u2705 \ud83d\udea7 AltCLIP \u2705 \u2705 \ud83d\udea7 Fuyu \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Open Flamingo \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 GPT-4 PaLM-2"},{"location":"supported-models/#roboflow-model-deployment-support","title":"Roboflow Model Deployment Support","text":"<p>You can optionally deploy some Target Models trained using Autodistill on Roboflow. Deploying on Roboflow allows you to use a range of concise SDKs for using your model on the edge, from roboflow.js for web deployment to NVIDIA Jetson devices.</p> <p>The following Autodistill Target Models are supported by Roboflow for deployment:</p> model name Supported? YOLOv8 Object Detection \u2705 YOLOv8 Instance Segmentation \u2705 YOLOv5 Object Detection \u2705 YOLOv5 Instance Segmentation \u2705 YOLOv8 Classification"},{"location":"what-model-should-i-use/","title":"What Model Should I Use?","text":"<p>With so many base models to use in labeling images, you may wonder \"what model should I use for labeling?\"</p>"},{"location":"what-model-should-i-use/#detection-and-segmentation","title":"Detection and Segmentation","text":"<p>We recommend using Grounding DINO as a starting point for detection, and Grounded SAM for segmentation.</p> <p>Grounding DINO is an effective zero-shot object detector that can identify a wide range of objects, from cars to vinyl record covers.</p> <p>Grounded SAM combines SAM with Grounding DINO to generate segmentation masks from Grounding DINO predictions.</p> <p>If Grounding DINO does not identify the object you want to label, consider experimenting with DETIC, which can identify over 20,000 classes of objects. DETIC supports an open vocabulary, so you can provide arbitrary text labels for objects.</p>"},{"location":"what-model-should-i-use/#classification","title":"Classification","text":"<p>We recommend using CLIP as a starting point for classification, which is effective at classifying a wide range of objects. Read the CLIP abstract from OpenAI to learn more.</p>"},{"location":"what-model-should-i-use/#roboflow-universe-models","title":"Roboflow Universe Models","text":"<p>You can use any of the 50,000+ pre-trained models on Roboflow Universe to auto-label data. Universe covers an extensive range of models, covering areas from logistics to agriculture.</p> <p>See the <code>autodistill-roboflow-universe</code> base model for more information.</p>"},{"location":"what-model-should-i-use/#understanding-other-models","title":"Understanding Other Models","text":"<p>The guidance above is a starting point, but there are many other models from which you can choose.</p> <p>Below is a list of all supported models not covered above, as well as notes about their usage.</p> <p>Some models may no longer be recommended because a new model surpasses its performance.</p>"},{"location":"what-model-should-i-use/#detection","title":"Detection","text":"<ul> <li>LLaVA-1.5: LLaVA 1.5 has significant memory requirements compared to other models. It may generalize well to a wide range of objects due to its language grounding, but more experimentation is needed.</li> <li>Kosmos-2: Kosmos-2, like LLaVA-1.5, has significant memory requirements compared to other models.</li> <li>OWL-ViT: We recommend using OWLv2 over OWL-ViT.</li> <li>CoDet: CoDet is a promising zero-shot detection model which we encourage you to try if Grounding DINO does not identify the objects you want to label.</li> <li>VLPart: VLPart is a promising zero-shot detection model which we encourage you to try if Grounding DINO does not identify the objects you want to label.</li> </ul>"},{"location":"what-model-should-i-use/#classification_1","title":"Classification","text":"<ul> <li>FastViT: FastViT can identify the classes in the ImageNet 1k dataset. FastViT has fast inference times, which makes its use ideal in applications where inference speed is critical.</li> <li>AltCLIP: AltCLIP reports strong zero-shot classification performance in English and Chinese when evaluated against the ImageNet dataset. This model may be useful if you want to provide Chinese prompts to auto-label images.</li> <li>DINOv2: An embedding model that may be useful for zero-shot classification.</li> <li>MetaCLIP: MetaCLIP is an open source CLIP model. It may be worth experimenting with if OpenAI's CLIP model does not perform well on your dataset.</li> <li>BLIP: BLIP is a zero-shot classifier. It has higher memory requirements than CLIP, but may perform better on some datasets.</li> <li>ALBEF: ALBEF is a zero-shot classifier. It has higher memory requirements than CLIP, but may perform better on some datasets.</li> </ul>"},{"location":"base_models/albef/","title":"ALBEF","text":"<p>Classification Base Model</p>"},{"location":"base_models/albef/#what-is-albef","title":"What is ALBEF?","text":"<p>ALBEF, developed by Salesforce, is a computer vision model that supports a range of tasks, including image-text pre-training, image-text retrieval, visual question anserting, and zero-shot classification.</p> <p>You can classify images using ALBEF with Autodistill.</p>"},{"location":"base_models/albef/#installation","title":"Installation","text":"<p>To use ALBEF with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-albef\n</code></pre>"},{"location":"base_models/albef/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_albef import ALBEF\n\n# define an ontology to map class names to our ALBEF prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = ALBEF(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/altclip/","title":"AltCLIP","text":"<p>Classification Base Model</p>"},{"location":"base_models/altclip/#what-is-altclip","title":"What is AltCLIP?","text":"<p>AltCLIP is a multi-modal vision model. With AltCLIP, you can compare the similarity between text and images, or the similarlity between two images. AltCLIP was trained on multi-lingual text-image pairs, which means it can be used for zero-shot classification with text prompts in different languages. Read the AltCLIP paper for more information.</p> <p>The Autodistill AltCLIP module enables you to use AltCLIP for zero-shot classification.</p>"},{"location":"base_models/altclip/#installation","title":"Installation","text":"<p>To use AltCLIP with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-altclip\n</code></pre>"},{"location":"base_models/altclip/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_altclip import AltCLIP\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our AltCLIP prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated results\n# then, load the model\nbase_model = AltCLIP(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\n\nresults = base_model.predict(\"construction.jpg\")\n\nprint(results)\n</code></pre>"},{"location":"base_models/altclip/#license","title":"License","text":"<p>The AltCLIP model is licensed under an Apache 2.0 license. See the model README for more information.</p>"},{"location":"base_models/altclip/#contributing","title":"\ud83c\udfc6 Contributing","text":"<p>We love your input! Please see the core Autodistill contributing guide to get started. Thank you \ud83d\ude4f to all our contributors!</p>"},{"location":"base_models/bioclip/","title":"BioCIP","text":"<p>Classification Base Model</p>"},{"location":"base_models/bioclip/#what-is-bioclip","title":"What is BioCLIP?","text":"<p>BioCLIP is a CLIP model trained on the TreeOfLife-10M dataset, created by the researchers who made BioCLIP. The dataset on which BioCLIP was trained included more than 450,000 classes.</p> <p>You can use BioCLIP to auto-label natural organisms (i.e. animals, plants) in images for use in training a classification model. You can combine this model with a grounded detection model to identify the exact region in which a given class is present in an image. Learn more about combining models with Autodistill.</p>"},{"location":"base_models/bioclip/#installation","title":"Installation","text":"<p>To use BioCLIP with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-bioclip\n</code></pre>"},{"location":"base_models/bioclip/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_bioclip import BioCLIP\n\n# define an ontology to map class names to our BioCLIP prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nclasses = [\"arabica\", \"robusta\"]\n\nbase_model = BioCLIP(\n    ontology=CaptionOntology(\n        {\n            item: item for item in classes\n        }\n    )\n)\n\nresults = base_model.predict(\"../arabica.jpeg\")\n\ntop = results.get_top_k(1)\ntop_class = classes[top[0][0]]\n\nprint(f\"Predicted class: {top_class}\")\n</code></pre>"},{"location":"base_models/bioclip/#license","title":"License","text":"<p>This project is licensed under an MIT license.</p> <p>The underlying BioCLIP model is also licensed under an MIT license.</p>"},{"location":"base_models/clip/","title":"CLIP","text":"<p>Classification Base Model</p>"},{"location":"base_models/clip/#what-is-clip","title":"What is CLIP?","text":"<p>CLIP, developed by OpenAI, is a computer vision model trained using pairs of images and text. You can use CLIP with autodistill for image classification.</p> <p>This project is licensed under an MIT license.</p>"},{"location":"base_models/clip/#installation","title":"Installation","text":"<p>To use CLIP with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-clip\n</code></pre>"},{"location":"base_models/clip/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_clip import CLIP\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = CLIP(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/codet/","title":"CoDet","text":"<p>Object Detection Base Model</p>"},{"location":"base_models/codet/#what-is-codet","title":"What is CoDet?","text":"<p>CoDet is an open vocabulary zero-shot object detection model. The model was described in the \"CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection\" published by Chuofan Ma, Yi Jiang, Xin Wen, Zehuan Yuan, Xiaojuan Qi. The paper was submitted to NeurIPS2023.</p>"},{"location":"base_models/codet/#installation","title":"Installation","text":"<p>To use CoDet with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-codet\n</code></pre>"},{"location":"base_models/codet/#quickstart","title":"Quickstart","text":"<p>When you first run the model, it will download CoDet and its dependencies, as well as the required model configuration and weights. The output during the download process will be verbose. If you stop the download process before it has finished, run <code>rm -rf ~/.cache/autodistill/CoDet</code> before running the model again. This ensures that you don't work from a part-installed CoDet setup.</p> <p>When the <code>predict()</code> function runs, the output will also be verbose. You can ignore the output printed to the console that appears when you call <code>predict()</code>.</p> <p>You can only predict classes in the LVIS vocabulary. You can see a list of supported classes in the <code>class_names.json</code> file in the autodistill-codet GitHub repository.</p> <p>Use the code snippet below to get started:</p> <pre><code>from autodistill_codet import CoDet\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\nimport cv2\n\n# define an ontology to map class names to our CoDet prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = CoDet(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\"\n        }\n    )\n)\n\n# run inference on an image and display the results\n# class_names is a list of all classes supported by the model\n# class_names can be used to turn the class_id values from the model into human-readable class names\n# class names is defined in self.class_names\npredictions = base_model.predict(\"./context_images/1.jpeg\")\nimage = cv2.imread(\"./context_images/1.jpeg\")\n\nplot(\n  image=image,\n  detections=predictions,\n  classes=base_model.class_names\n)\n\n# run inference on a folder of images and save the results\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/codet/#license","title":"License","text":"<p>This project is licensed under an Apache 2.0 license, except where files explicitly note a license.</p>"},{"location":"base_models/detic/","title":"DETIC","text":"<p>Object Detection Base Model</p>"},{"location":"base_models/detic/#what-is-detic","title":"What is DETIC?","text":"<p>DETIC is a transformer-based object detection and segmentation model developed by Meta Research.</p>"},{"location":"base_models/detic/#installation","title":"Installation","text":"<p>To use DETIC with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-detic\n</code></pre>"},{"location":"base_models/detic/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_detic import DETIC\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our DETIC prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = DETIC(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpg\")\n</code></pre>"},{"location":"base_models/efficient-yolo-world/","title":"Efficient YOLO-World","text":"<p>Segmentation Base Model</p>"},{"location":"base_models/efficient-yolo-world/#what-is-efficient-yolo-world","title":"What is Efficient YOLO-World?","text":"<p>EfficientYOLOWorld is a combination of two models:</p> <ol> <li>YOLO-World, a zero-shot object detection model, and;</li> <li>EfficientSAM, an image segmentation model.</li> </ol> <p>This model runs EfficientSAM on each bounding box region generated by YOLO-World. This allows you to retrieve both the bounding box and the segmentation mask for each object of interest in an image.</p>"},{"location":"base_models/efficient-yolo-world/#installation","title":"Installation","text":"<p>To use EfficientYOLOWorld with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-efficient-yolo-world\n</code></pre>"},{"location":"base_models/efficient-yolo-world/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_efficient_yolo_world import EfficientYOLOWorld\nfrom autodistill.detection import CaptionOntology\nimport cv2\nimport supervision as sv\n\n# define an ontology to map class names to our EfficientYOLOWorld prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = EfficientYOLOWorld(ontology=CaptionOntology({\"book\": \"book\"}))\n\n# predict on an image\nresult = base_model.predict(\"bookshelf.jpeg\", confidence=0.1)\n\nimage = cv2.imread(\"bookshelf.jpeg\")\n\nmask_annotator = sv.MaskAnnotator()\nannotated_frame = mask_annotator.annotate(\n    scene=image.copy(),\n    detections=result,\n)\n\nsv.plot_image(annotated_frame)\n\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/efficient-yolo-world/#license","title":"License","text":"<p>EfficientSAM is licensed under an Apache 2.0 license.</p> <p>YOLO-World is licensed under a GPL-3.0 license.</p>"},{"location":"base_models/efficientsam/","title":"EfficientSAM","text":"<p>Segmentation Base Model</p>"},{"location":"base_models/efficientsam/#what-is-efficientsam","title":"What is EfficientSAM?","text":"<p>This repository contains the code supporting the EfficientSAM base model for use with Autodistill.</p> <p>EfficientSAM is an image segmentation model that was introduced in the paper \"EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything\". You can use EfficientSAM with autodistill for image segmentation.</p>"},{"location":"base_models/efficientsam/#installation","title":"Installation","text":"<p>To use EfficientSAM with Autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-efficientsam\n</code></pre>"},{"location":"base_models/efficientsam/#quickstart","title":"Quickstart","text":"<p>This model returns segmentation masks for all objects in an image.</p> <p>If you want segmentation masks only for specific objects matching a text prompt, we recommend combining EfficientSAM with a zero-shot detection model like GroundingDINO.</p> <p>Read our ComposedDetectionModel documentation for more information about how to combine models like EfficientSAM and GroundingDINO.</p> <pre><code>from autodistill_efficientsam import EfficientSAM\n\n# define an ontology to map class names to our EfficientSAM prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = EfficientSAM(None)\n\nmasks = base_model.predict(\"./image.png\")\n</code></pre>"},{"location":"base_models/efficientsam/#license","title":"License","text":"<p>This project is licensed under an Apache 2.0 license.</p>"},{"location":"base_models/evaclip/","title":"EvaCLIP","text":"<p>Classification Base Model Community Contribution</p>"},{"location":"base_models/evaclip/#what-is-evaclip","title":"What is EvaCLIP?","text":"<p>Note: This module was contributed by a third-party community member unaffiliated with Roboflow. </p> <p>This repository contains the code supporting the EvaCLIP base model for use with Autodistill.</p> <p>EvaCLIP, is a computer vision model trained using pairs of images and text. It can be used for classification of images.</p>"},{"location":"base_models/evaclip/#installation","title":"Installation","text":"<p>To use EvaCLIP with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-evaclip\n</code></pre>"},{"location":"base_models/evaclip/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_evaclip import EvaCLIP\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our EvaCLIP prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = EvaCLIP(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\n\nresults = base_model.predict(\"./context_images/test.jpg\")\n\nprint(results)\n\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/fastsam/","title":"FastSAM","text":"<p>Object Detection Base Model</p>"},{"location":"base_models/fastsam/#what-is-fastsam","title":"What is FastSAM?","text":"<p>FastSAM is a segmentation model trained on 2% of the SA-1B dataset used to train the Segment Anything Model.</p> <p>Read the full Autodistill documentation.</p> <p>Read the FastSAM Autodistill documentation.</p>"},{"location":"base_models/fastsam/#installation","title":"Installation","text":"<p>To use FastSAM with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-fastsam\n</code></pre>"},{"location":"base_models/fastsam/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_fastsam import FastSAM\n\n# define an ontology to map class names to our FastSAM prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = FastSAM(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/gemini/","title":"Gemini","text":"<p>Classification Base Model</p>"},{"location":"base_models/gemini/#what-is-gemini","title":"What is Gemini?","text":"<p>This repository contains the code supporting the Gemini base model for use with Autodistill.</p> <p>Gemini, family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio.s.</p>"},{"location":"base_models/gemini/#installation","title":"Installation","text":"<p>To use Gemini with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-gemini\n</code></pre>"},{"location":"base_models/gemini/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_gemini import Gemini\n\n# define an ontology to map class names to our Gemini prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = Gemini(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    ),\n    gcp_region=\"us-central1\",\n    gcp_project=\"project-name\",\n    model=\"gemini-1.5-flash\"\n)\n\n# run inference on an image\nresult = base_model.predict(\"image.jpg\")\n\nprint(result)\n\n# label a folder of images\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/gpt4o/","title":"GPT-4o","text":"<p>Object Detection Base Model</p>"},{"location":"base_models/gpt4o/#what-is-gpt-4o","title":"What is GPT-4o?","text":"<p>GPT-4o, developed by OpenAI, is a multi-modal language model that works across the image, text, and audio domains. With GPT-4o, you can ask questions about images in natural language. The <code>autodistill-gpt4o</code> module enables you to classify images using GPT-4V.</p> <p>This model uses the gpt-4-o API announced by OpenAI on May 13th, 2024.</p> <p>[!NOTE] Using this project will incur billing charges for API calls to the OpenAI GPT-4 Vision API. Refer to the OpenAI pricing page for more information and to calculate your expected pricing. This package makes one API call per image you want to label.</p>"},{"location":"base_models/gpt4o/#installation","title":"Installation","text":"<p>To use GPT-4o with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-gpt-4o\n</code></pre>"},{"location":"base_models/gpt4o/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_gpt_4o import GPT4o\n\n# define an ontology to map class names to our GPT-4o prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = GPT4o(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    ),\n    api_key=\"OPENAI_API_KEY\"\n)\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/gpt4o/#license","title":"License","text":"<p>This project is licensed under an MIT license.</p>"},{"location":"base_models/gpttext/","title":"GPT and LLaMAfile","text":"<p>Text Classification Base Model</p>"},{"location":"base_models/gpttext/#what-is-distilbert","title":"What is DistilBERT?","text":"<p>You can use Autodistill GPT to classify text using OpenAI's GPT models for use in training smaller, fine-tuned text classification models. You can also use Autodistill GPT to use LLaMAfile text generation models.</p> <p>Read the full Autodistill documentation.</p>"},{"location":"base_models/gpttext/#installation","title":"Installation","text":"<p>To use GPT or LLaMAfile models with Autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-gpt-text\n</code></pre>"},{"location":"base_models/gpttext/#quickstart-llamafile","title":"Quickstart (LLaMAfile)","text":"<pre><code>from autodistill_gpt_text import GPTClassifier\n\n# define an ontology to map class names to our GPT prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = GPTClassifier(\n    ontology=CaptionOntology(\n        {\n            \"computer vision\": \"computer vision\",\n            \"natural language processing\": \"nlp\"\n        }\n    ),\n    base_url = \"http://localhost:8080/v1\", # your llamafile server\n    model_id=\"LLaMA_CPP\"\n)\n\n# label a single text\nresult = GPTClassifier.predict(\"This is a blog post about computer vision.\")\n\n# label a JSONl file of texts\nbase_model.label(\"data.jsonl\", output=\"output.jsonl\")\n</code></pre>"},{"location":"base_models/gpttext/#quickstart-gpt","title":"Quickstart (GPT)","text":"<pre><code>from autodistill_gpt_text import GPTClassifier\n\n# define an ontology to map class names to our GPT prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = GPTClassifier(\n    ontology=CaptionOntology(\n        {\n            \"computer vision\": \"computer vision\",\n            \"natural language processing\": \"nlp\"\n        }\n    )\n)\n\n# label a single text\nresult = GPTClassifier.predict(\"This is a blog post about computer vision.\")\n\n# label a JSONl file of texts\nbase_model.label(\"data.jsonl\", output=\"output.jsonl\")\n</code></pre> <p>The output JSONl file will contain all the data in your original file, with a new <code>classification</code> key in each entry that contains the predicted text label associated with that entry.</p>"},{"location":"base_models/grounded-edgesam/","title":"Grounded Edge SAM","text":"<p>Segmentation Base Model</p>"},{"location":"base_models/grounded-edgesam/#what-is-evaclip","title":"What is EvaCLIP?","text":"<p>EdgeSAM, introduced in the \"EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM\" paper, is a faster version of the Segment Anything model.</p> <p>Grounded EdgeSAM combines Grounding DINO and EdgeSAM, allowing you to identify objects and generate segmentation masks for them.</p>"},{"location":"base_models/grounded-edgesam/#installation","title":"Installation","text":"<p>To use Grounded EdgeSAM with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-grounded-edgesam\n</code></pre>"},{"location":"base_models/grounded-edgesam/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_clip import CLIP\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nfrom autodistill_grounded_edgesam import GroundedEdgeSAM\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\nimport cv2\n\n# define an ontology to map class names to our GroundedSAM prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = GroundedEdgeSAM(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"forklift\": \"forklift\",\n        }\n    )\n)\n\n# run inference on a single image\nresults = base_model.predict(\"logistics.jpeg\")\n\nplot(\n    image=cv2.imread(\"logistics.jpeg\"),\n    classes=base_model.ontology.classes(),\n    detections=results\n)\n\n# label a folder of images\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/grounded-edgesam/#license","title":"License","text":"<p>This repository is released under an S-Lab License 1.0 license.</p>"},{"location":"base_models/grounded-sam-2/","title":"Grounded SAM 2","text":"<p>Image Segmentation Base Model</p>"},{"location":"base_models/grounded-sam-2/#grounded-sam-2-base-model","title":"Grounded SAM 2 Base Model","text":"<p>This repository contains the code implementing Grounded SAM 2 using Florence-2 as a grounding model and Segment Anything 2 as a segmentation model for use with <code>autodistill</code>.</p> <p>Florence-2 is a zero-shot multimodal model. You can use Florence-2 for open vocabulary object detection. This project uses the object detection capabilities in Florence-2 to ground the SAM 2 model.</p>"},{"location":"base_models/grounded-sam-2/#installation","title":"Installation","text":"<p>To use the Grounded SAM 2 base model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-grounded-sam-2\n</code></pre>"},{"location":"base_models/grounded-sam-2/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_grounded_sam_2 import GroundedSAM2\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\nimport cv2\n\n# define an ontology to map class names to our Grounded SAM 2 prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = GroundedSAM2(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"shipping container\": \"shipping container\",\n        }\n    )\n)\n\n# run inference on a single image\nresults = base_model.predict(\"logistics.jpeg\")\n\nplot(\n    image=cv2.imread(\"logistics.jpeg\"),\n    classes=base_model.ontology.classes(),\n    detections=results\n)\n# label all images in a folder called `context_images`\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/grounded-sam-2/#license","title":"License","text":"<p>The code in this repository is licensed under an Apache 2.0 license.</p>"},{"location":"base_models/grounded-sam-2/#contributing","title":"\ud83c\udfc6 Contributing","text":"<p>We love your input! Please see the core Autodistill contributing guide to get started. Thank you \ud83d\ude4f to all our contributors!</p>"},{"location":"base_models/groundedsam/","title":"Grounded SAM","text":"<p>Object Detection Base Model</p>"},{"location":"base_models/groundedsam/#what-is-grounded-sam","title":"What is Grounded SAM?","text":"<p>Grounded SAM uses the Segment Anything Model to identify objects in an image and assign labels to each image.</p>"},{"location":"base_models/groundedsam/#installation","title":"Installation","text":"<p>To use the Grounded SAM base model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-grounded-sam\n</code></pre>"},{"location":"base_models/groundedsam/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our Grounded SAM prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = GroundedSAM(ontology=CaptionOntology({\"shipping container\": \"container\"}))\n\n# label all images in a folder called `context_images`\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/grounding-dino/","title":"Grounding DINO","text":"<p>Object Detection Base Model</p>"},{"location":"base_models/grounding-dino/#what-is-grounding-dino","title":"What is Grounding DINO?","text":"<p>Grounding DINO is a zero-shot object detection model developed by IDEA Research. You can distill knowledge from Grounding DINO into a smaller model using Autodistill.</p> <p>Tip</p> <p>You can use Grounding DINO to auto-label images without any code with the Roboflow Auto Label product.</p>"},{"location":"base_models/grounding-dino/#installation","title":"Installation","text":"<p>To use the Grounded dino base model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-grounding-dino\n</code></pre>"},{"location":"base_models/grounding-dino/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_grounding_dino import GroundingDINO\nfrom autodistill.detection import CaptionOntology\n\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = GroundingDINO(ontology=CaptionOntology({\"shipping container\": \"container\"}))\n\n# label all images in a folder called `context_images`\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/hls-geospatial/","title":"HLS Geospatial","text":"<p>Segmentation Base Model</p>"},{"location":"base_models/hls-geospatial/#what-is-hls-geospatial","title":"What is HLS Geospatial?","text":"<p>This repository contains the code supporting the HLS Geospatial base model for use with Autodistill.</p> <p>Harmonized Landsat and Sentinel-2 (HLS) Prithvi is a collection of foundation models for geospatial analysis, developed by NASA and IBM. You can use Autodistill to automatically label images for use in training segmentation models.</p> <p>The following models are supported:</p> <ul> <li>Prithvi-100M-sen1floods11</li> </ul> <p>This module accepts <code>tiff</code> files as input and returns segmentation masks.</p>"},{"location":"base_models/hls-geospatial/#installation","title":"Installation","text":"<p>To use HLS Geospatial with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-hls-geospatial\n</code></pre>"},{"location":"base_models/hls-geospatial/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_hls_geospatial import HLSGeospatial\nimport numpy as np\nimport rasterio\nfrom skimage import exposure\nimport supervision as sv\n\ndef stretch_rgb(rgb):\n    ls_pct = 1\n    pLow, pHigh = np.percentile(rgb[~np.isnan(rgb)], (ls_pct, 100 - ls_pct))\n    img_rescale = exposure.rescale_intensity(rgb, in_range=(pLow, pHigh))\n\n    return img_rescale\n\n\n#replace with the name of the file you want to label\nFILE_NAME = \"USA_430764_S2Hand.tif\"\n\nwith rasterio.open(FILE_NAME) as src:\n    image = src.read()\n\n    mask = image\n\n    rgb = stretch_rgb(\n        (mask[[3, 2, 1], :, :].transpose((1, 2, 0)) / 10000 * 255).astype(np.uint8)\n    )\n\n    base_model = HLSGeospatial()\n\n    # replace with the file you want to use\n    detections = base_model.predict(FILE_NAME)\n\n    mask_annotator = sv.MaskAnnotator()\n\n    annotated_image = mask_annotator.annotate(scene=rgb, detections=detections)\n\n    sv.plot_image(annotated_image, size=(10, 10))\n\n# label a folder of .tif files\nbase_model.label(\"./context_images\", extension=\".tif\")\n</code></pre>"},{"location":"base_models/hls-geospatial/#license","title":"License","text":"<p>This project is licensed under an Apache 2.0 license.</p>"},{"location":"base_models/metaclip/","title":"MetaCLIP","text":"<p>Object Detection Base Model</p>"},{"location":"base_models/metaclip/#what-is-metaclip","title":"What is MetaClip?","text":"<p>MetaCLIP, developed by Meta AI Research, is a computer vision model trained using pairs of images and text. The model was described in the Demystifying CLIP Data paper. You can use MetaCLIP with autodistill for image classification.</p>"},{"location":"base_models/metaclip/#installation","title":"Installation","text":"<p>To use MetaCLIP with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-metaclip\n</code></pre>"},{"location":"base_models/metaclip/#quickstart","title":"Quickstart","text":""},{"location":"base_models/metaclip/#get-predictions","title":"get predictions","text":"<pre><code>from autodistill_metaclip import MetaCLIP\n\n# define an ontology to map class names to our MetaCLIP prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = MetaCLIP(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\n\nresults = base_model.predict(\"./image.png\")\nprint(results)\n</code></pre>"},{"location":"base_models/metaclip/#calculate-and-compare-embeddings","title":"calculate and compare embeddings","text":"<pre><code>from autodistill_metaclip import MetaCLIP\n\nbase_model = MetaCLIP(None)\n\ntext = base_model.embed_text(\"coffee\")\nimage = base_model.embed_image(\"coffeeshop.jpg\")\n\nprint(base_model.compare(text, image))\n</code></pre>"},{"location":"base_models/owlvit/","title":"OWL-ViT","text":"<p>Object Detection Base Model</p> <p>This model has a newer version: OWLv2. We recommend using OWLv2 for better performance.</p>"},{"location":"base_models/owlvit/#what-is-owl-vit","title":"What is OWL-ViT?","text":"<p>OWL-ViT is a transformer-based object detection model developed by Google Research.</p>"},{"location":"base_models/owlvit/#installation","title":"Installation","text":"<p>To use OWL-ViT with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-owl-vit\n</code></pre>"},{"location":"base_models/owlvit/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_owl_vit import OWLViT\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our OWLViT prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = OWLViT(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpg\")\n</code></pre>"},{"location":"base_models/paligemma/","title":"PaliGemma","text":"<p>Object Detection Base Model</p>"},{"location":"base_models/paligemma/#what-is-paligemma","title":"What is PaliGemma?","text":"<p>PaLiGemma, developed by Google, is a computer vision model trained using pairs of images and text.</p> <p>You can label data with PaliGemma models for use in training smaller, fine-tuned models with Autodisitll.</p> <p>You can also fine-tune PaliGemma models with Autodistill.</p>"},{"location":"base_models/paligemma/#installation","title":"Installation","text":"<p>To use PaLiGemma with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-paligemma\n</code></pre>"},{"location":"base_models/paligemma/#quickstart","title":"Quickstart","text":""},{"location":"base_models/paligemma/#auto-label-with-an-existing-model","title":"Auto-label with an existing model","text":"<pre><code>from autodistill_paligemma import PaliGemma\n\n# define an ontology to map class names to our PaliGemma prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = PaliGemma(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\n\n# label a single image\nresult = PaliGemma.predict(\"test.jpeg\")\nprint(result)\n\n# label a folder of images\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/paligemma/#model-fine-tuning","title":"Model fine-tuning","text":"<p>You can fine-tune PaliGemma models with LoRA for deployment with Roboflow Inference.</p> <p>To train a model, use this code:</p> <pre><code>from autodistill_paligemma import PaLiGemmaTrainer\n\ntarget_model = PaLiGemmaTrainer()\ntarget_model.train(\"./data/\")\n\nresult = target_model.predict(\"test.jpeg\")\nprint(result)\n</code></pre>"},{"location":"base_models/paligemma/#license","title":"License","text":"<p>The model weights for PaLiGemma are licensed under a custom Google license. To learn more, refer to the Google Gemma Terms of Use.</p>"},{"location":"base_models/remoteclip/","title":"RemoteCLIP","text":"<p>Classification Base Model</p>"},{"location":"base_models/remoteclip/#what-is-remoteclip","title":"What is RemoteCLIP?","text":"<p>RemoteCLIP is a vision-language CLIP model trained on remote sensing data. According to the RemoteCLIP README:</p> <p>RemoteCLIP outperforms previous SoTA by 9.14% mean recall on the RSICD dataset and by 8.92% on RSICD dataset. For zero-shot classification, our RemoteCLIP outperforms the CLIP baseline by up to 6.39% average accuracy on 12 downstream datasets.</p>"},{"location":"base_models/remoteclip/#installation","title":"Installation","text":"<p>To use RemoteCLIP with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-remote-clip\n</code></pre>"},{"location":"base_models/remoteclip/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_remote_clip import RemoteCLIP\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our RemoteCLIP prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = RemoteCLIP(\n    ontology=CaptionOntology(\n        {\n            \"airport runway\": \"runway\",\n            \"countryside\": \"countryside\",\n        }\n    )\n)\n\npredictions = base_model.predict(\"runway.jpg\")\n\nprint(predictions)\n</code></pre>"},{"location":"base_models/remoteclip/#license","title":"License","text":"<p>This project is covered under an Apache 2.0 license.</p>"},{"location":"base_models/sam-clip/","title":"Sam clip","text":"<p>Segmentation Base Model</p>"},{"location":"base_models/sam-clip/#what-is-sam-clip","title":"What is SAM-CLIP?","text":"<p>SAM-CLIP uses the Segment Anything Model to identify objects in an image and assign labels to each image. Then, CLIP is used to find masks that are related to the given prompt.</p>"},{"location":"base_models/sam-clip/#installation","title":"Installation","text":"<p>To use the SAM-CLIP base model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-sam-clip\n</code></pre>"},{"location":"base_models/sam-clip/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_sam_clip import SAMCLIP\nfrom autodistill.detection import CaptionOntology\n\n\n# define an ontology to map class names to our CLIP prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = SAMCLIP(ontology=CaptionOntology({\"shipping container\": \"container\"}))\n\n# label all images in a folder called `context_images`\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/samhq/","title":"SAM HQ","text":"<p>Segmentation Base Model</p>"},{"location":"base_models/samhq/#what-is-segment-anything-hq","title":"What is Segment Anything HQ?","text":"<p>SAM HQ is a zero-shot segmentation model capable of producing detailed masks, developed by ETH VIS. SAM HQ can segment an entire image into masks, or use points to segment specific parts of an object. You can use Segment Anything with Autodistill to segment objects. Segment Anything does not assign classes, so you should use SAM HQ model with a tool like Grounding DINO or GPT-4V.</p> <p>Read the full Autodistill documentation.</p> <p>Read the SAM HQ Autodistill documentation.</p>"},{"location":"base_models/samhq/#installation","title":"Installation","text":"<p>To use SAM HQ with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-sam-hq\n</code></pre>"},{"location":"base_models/samhq/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_sam_hq import HQSAM\n\nbase_model = HQSAM(None)\n\nmasks = base_model.predict(\"./image.jpeg\")\n\nprint(masks)\n</code></pre>"},{"location":"base_models/samhq/#license","title":"License","text":"<p>This project is licensed under an Apache 2.0 license.</p>"},{"location":"base_models/seggpt/","title":"SegGPT","text":"<p>Segmentation Base Model</p>"},{"location":"base_models/seggpt/#what-is-seggpt","title":"What is SegGPT?","text":"<p>SegGPT is a transformer-based, few-shot semantic segmentation model developed by BAAI Vision.</p> <p>This model performs well on task-specific segmentation tasks when given a few labeled images from which to learn features about the objects you want to identify.</p>"},{"location":"base_models/seggpt/#installation","title":"Installation","text":"<p>To use SegGPT with Autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-seggpt\n</code></pre>"},{"location":"base_models/seggpt/#about-seggpt","title":"About SegGPT","text":"<p>SegGPT performs \"in-context\" segmentation. This means it requires a handful of pre-labelled \"context\" images.</p> <p>You will need some labeled images to use SegGPT. Don't have any labeled images? Check out Roboflow Annotate, a feature-rich annotation tool from which you can export data for use with Autodistill.</p>"},{"location":"base_models/seggpt/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_seggpt import SegGPT, FewShotOntology\n\nbase_model = SegGPT(\n    ontology=FewShotOntology(supervision_dataset)\n)\n\nbase_model.label(\"./unlabelled-climbing-photos\", extension=\".jpg\")\n</code></pre>"},{"location":"base_models/seggpt/#how-to-load-data-from-roboflow","title":"How to load data from Roboflow","text":"<p>Labelling and importing images is easy!</p> <p>You can use Roboflow Annotate to label a few images (5-10 should work fine). For your Project Type, make sure to pick Instance Segmentation, as you will be labelling with polygons.</p> <p>Once you have labelled your images, you can press Generate &gt; Generate New Version. You can use all the default options--no Augmentations are necessary.</p> <p>Once your dataset version is generated, you can press Export &gt; Continue.</p> <p>Then you will get some download code to copy. It should look something like this:</p> <pre><code>!pip install roboflow\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"ABCDEFG\")\nproject = rf.workspace(\"lorem-ipsum\").project(\"dolor-sit-amet\")\ndataset = project.version(1).download(\"yolov8\")\n</code></pre> <p>Note: if you are not using a notebook environment, you should remove <code>!pip install roboflow</code> from your code, and run <code>pip install roboflow</code> in your terminal instead.</p> <p>To import your dataset into Autodistill, run the following:</p> <pre><code>import supervision as sv\n\nsupervision_dataset = sv.DetectionDataset.from_yolo(\n    images_directory_path=f\"{dataset.location}/train/images\",\n    annotations_directory_path=f\"{dataset.location}/train/labels\",\n    data_yaml_path=f\"{dataset.location}/data.yaml\"\n)\n</code></pre>"},{"location":"base_models/siglip/","title":"SigLIP","text":"<p>Classification Base Model</p>"},{"location":"base_models/siglip/#what-is-albef","title":"What is ALBEF?","text":"<p>SigLIP is an image classification and embedding model architecture first introduced in the paper \"Sigmoid Loss for Language Image Pre-Training\".</p> <p>You can use SigLIP to classify images with Autodistill.</p>"},{"location":"base_models/siglip/#installation","title":"Installation","text":"<p>To use SigLIP with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-siglip\n</code></pre>"},{"location":"base_models/siglip/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_siglip import SigLIP\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our SigLIP prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nlabels = [\"person\", \"a forklift\"]\nbase_model = SigLIP(\n    ontology=CaptionOntology({item: item for item in labels})\n)\n\nresults = base_model.predict(\"image.jpeg\", confidence=0.1)\n\ntop_1 = results.get_top_k(1)\n\n# show top label\nprint(labels[top_1[0][0]])\n\n# label folder of images\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/siglip/#license","title":"License","text":"<p>The SigLIP model is licensed under an Apache 2.0 license.</p>"},{"location":"base_models/universe/","title":"Roboflow Universe","text":"<p>Object Detection Segmentation Base Model</p>"},{"location":"base_models/universe/#what-is-roboflow-universe","title":"What is Roboflow Universe?","text":"<p>Roboflow Universe is a community where people share computer vision models and datasets. Over 50,000 models and 250,000 datasets have been shared on Universe, with new models available every day. You can use Autodistill to run object detection and segmentation models hosted on Roboflow Universe.</p> <p>[!NOTE] Using this project will use Roboflow API calls. You will need a free Roboflow account to use this project. Sign up for a free Roboflow account to get started. Learn more about pricing.</p> <p>Read the full Autodistill documentation.</p> <p>Read the Roboflow Universe Autodistill documentation.</p>"},{"location":"base_models/universe/#installation","title":"Installation","text":"<p>To use models hosted on Roboflow Universe with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-roboflow-universe\n</code></pre>"},{"location":"base_models/universe/#quickstart","title":"Quickstart","text":"<p>[!NOTE] Autodistill uses ontology to map model predictions to the expected class labels. For other Autodistill models, the term 'caption' is used when the model accepts prompting or a description for a prediction. When using Roboflow Universe as an Autodistill base model, the 'caption' will be the class name/label that the Universe model will return. </p> <pre><code>from autodistill_roboflow_universe import RoboflowUniverseModel\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\nimport cv2\n\n# define an ontology to map class names to our Roboflow model prompt:\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved in the generated annotations\n\nmodel_configs = [\n    (\"PROJECT_ID\", VERSION_NUMBER)\n]\n\nbase_model = RoboflowUniverseModel(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"forklift\": \"vehicle\"\n        }\n),\n    api_key=\"ROBOFLOW_API_KEY\",\n    model_configs=model_configs,\n)\n\n# run inference on a single image\nresult = base_model.predict(\"image.jpeg\")\n\nprint(result)\n\nplot(\n    image=cv2.imread(\"image.jpeg\"),\n    detections=result,\n    classes=base_model.ontology.classes(),\n)\n\n# label a folder of images\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre> <p>Above, replace:</p> <ul> <li><code>API_KEY</code>: with your Roboflow API key</li> <li><code>PROJECT_NAME</code>: with your Roboflow project ID.</li> <li><code>VERSION</code>: with your Roboflow model version.</li> <li><code>model_type</code>: with the type of model you want to run. Options are <code>object-detection</code>, <code>classification</code>, or <code>segmentation</code>. This value must be the same as the model type trained on Roboflow Universe.</li> </ul> <p>You can run multiple models on a single image. This is ideal if you need to identify multiple objects using different models hosted on Roboflow Universe. To run multiple models, add the models you want to run in the <code>model_configs</code> list. For example:</p> <pre><code>model_configs = [\n    (\"PROJECT_ID\", VERSION_NUMBER),\n    (\"PROJECT_ID\", VERSION_NUMBER)\n]\n</code></pre> <p>All models will be run on every image.</p> <p>Learn how to retrieve your Roboflow API key. Learn how to retrieve a model ID.</p>"},{"location":"base_models/universe/#license","title":"License","text":"<p>This project is licensed under an MIT license.</p>"},{"location":"base_models/universe/#contributing","title":"\ud83c\udfc6 Contributing","text":"<p>We love your input! Please see the core Autodistill contributing guide to get started. Thank you \ud83d\ude4f to all our contributors!</p>"},{"location":"base_models/vlpart/","title":"VLPart","text":"<p>Object Detection Base Model</p>"},{"location":"base_models/vlpart/#what-is-vlpart","title":"What is VLPart?","text":"<p>VLPart, developed by Meta Research, is an object detection and segmentation model that works with an open vocabulary. <code>autodistill-vlpart</code> enables you to use VLPart to auto-label images for use in training a fine-tuned model. <code>autodistill-vlpart</code> supports the LVIS vocabulary.</p>"},{"location":"base_models/vlpart/#installation","title":"Installation","text":"<p>To use VLPart with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-vlpart\n</code></pre>"},{"location":"base_models/vlpart/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_vlpart import VLPart\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\n\n# define an ontology to map class names to our VLPart prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = VLPart(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\"\n        }\n    )\n)\n\npredictions = base_model.predict(\"./image.png\")\n\nprint(predictions)\n\nplot(\n    image=cv2.imread(\"./image.png\"),\n    classes=base_model.class_names,\n    detections=predictions\n)\n\n# label the images in the context_images folder\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/vlpart/#license","title":"License","text":"<p>This project is licensed under an MIT license.</p>"},{"location":"base_models/yolo-world/","title":"YOLO-World","text":"<p>Object Detection Base Model</p>"},{"location":"base_models/yolo-world/#what-is-yolo-world","title":"What is YOLO-World?","text":"<p>YOLO-World is a YOLO-based open vocabulary model for open vocabulary detection.</p> <p>YOLO-World was developed by Tencent's AI Lab.</p>"},{"location":"base_models/yolo-world/#installation","title":"Installation","text":"<p>To use YOLO-World with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-yolo-world\n</code></pre>"},{"location":"base_models/yolo-world/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_yolo_world import YOLOWorldModel\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\nimport cv2\n\n# define an ontology to map class names to our GroundedSAM prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = YOLOWorldModel(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"car\": \"car\",\n        }\n    ),\n    model_type = \"yolov8s-world.pt\"\n)\n\n# run inference on a single image\nresults = base_model.predict(\"assets/test.jpg\")\n\nplot(\n    image=cv2.imread(\"assets/test.jpg\"),\n    classes=base_model.ontology.classes(),\n    detections=results\n)\n# label all images in a folder called `context_images`\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"reference/","title":"Index","text":"<p>This section of the Autodistill documentation covers the low-level Autodistill API. This section may be useful for advanced users who want to understand the Autodisitll API in more depth.</p>"},{"location":"reference/#autodistill-api-in-a-nutshell","title":"Autodistill API, in a Nutshell","text":"<p>The Autodistill API consists of three concepts:</p> <ol> <li>Base models, which are used to auto-label data;</li> <li>Target models, which are trained on the auto-labeled data; and</li> <li>Ontologies, which tell Autodistill what you want to identify and what labels should be called in your dataset.</li> </ol> <p>There are three different kinds of base models:</p> <ol> <li>Detection models, which identify objects in images and return bounding boxes and/or segmentation masks;</li> <li>Classification models, which classify images and return a class label; and;</li> <li>Embedding models, which return semantic embeddings for images. These are used with the <code>EmbeddingOntology</code> class.</li> </ol> <p>The base model you use depends on the type of data you want to label. You can also combine models using the ComposedDetectionModel API, which allows you to refine labels from detection models.</p> <p>There are two different kinds of target models:</p> <ol> <li>Detection models, and;</li> <li>Classification models.</li> </ol> <p>Autodistill does not support training embedding models.</p>"},{"location":"reference/utilities/","title":"Utilities","text":"<p>Learn about utility functions available for use with Autodistill.</p>"},{"location":"reference/utilities/#plot-an-image-with-predictions","title":"Plot an Image with Predictions","text":"<p>Plot bounding boxes or segmentation masks on an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>The image to plot on</p> required <code>detections</code> <p>The detections to plot</p> required <code>classes</code> <code>List[str]</code> <p>The classes to plot</p> required <code>raw</code> <p>Whether to return the raw image or plot it interactively</p> <code>False</code> <p>Returns:</p> Type Description <p>The raw image (np.ndarray) if raw=True, otherwise None (image is plotted interactively</p> Source code in <code>autodistill/utils.py</code> <pre><code>def plot(image: np.ndarray, detections, classes: List[str], raw=False):\n\"\"\"\n    Plot bounding boxes or segmentation masks on an image.\n\n    Args:\n        image: The image to plot on\n        detections: The detections to plot\n        classes: The classes to plot\n        raw: Whether to return the raw image or plot it interactively\n\n    Returns:\n        The raw image (np.ndarray) if raw=True, otherwise None (image is plotted interactively\n    \"\"\"\n    # TODO: When we have a classification annotator\n    # in supervision, we can add it here\n    if detections.mask is not None:\n        annotator = sv.MaskAnnotator()\n    else:\n        annotator = sv.BoxAnnotator()\n\n    label_annotator = sv.LabelAnnotator()\n\n    labels = [\n        f\"{classes[class_id]} {confidence:0.2f}\"\n        for _, _, confidence, class_id, _, _ in detections\n    ]\n\n    annotated_frame = annotator.annotate(scene=image.copy(), detections=detections)\n    annotated_frame = label_annotator.annotate(\n        scene=annotated_frame, labels=labels, detections=detections\n    )\n\n    if raw:\n        return annotated_frame\n\n    sv.plot_image(annotated_frame, size=(8, 8))\n</code></pre>"},{"location":"reference/utilities/#compare-models","title":"Compare Models","text":"<p>Compare the predictions of multiple models on multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list</code> <p>The models to compare</p> required <code>images</code> <code>List[str]</code> <p>The images to compare</p> required <p>Returns:</p> Type Description <p>A grid of images with the predictions of each model on each image.</p> Source code in <code>autodistill/utils.py</code> <pre><code>def compare(models: list, images: List[str]):\n\"\"\"\n    Compare the predictions of multiple models on multiple images.\n\n    Args:\n        models: The models to compare\n        images: The images to compare\n\n    Returns:\n        A grid of images with the predictions of each model on each image.\n    \"\"\"\n    image_results = []\n    model_results = []\n\n    for model in models:\n        # get model class name\n        model_name = model.__class__.__name__\n\n        for image in images:\n            results = model.predict(image)\n\n            image_data = cv2.imread(image)\n\n            image_result = plot(\n                image_data, results, classes=model.ontology.prompts(), raw=True\n            )\n\n            image_results.append(image_result)\n\n            model_results.append(model_name)\n\n    sv.plot_images_grid(\n        image_results,\n        grid_size=(len(models), len(images)),\n        titles=model_results,\n        size=(16, 16),\n    )\n</code></pre>"},{"location":"reference/utilities/#load-an-image","title":"Load an Image","text":"<p>Load an image from a file path, URI, PIL image, or numpy array.</p> <p>This function is for use by Autodistill modules. You don't need to use it directly.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to load</p> required <code>return_format</code> <p>The format to return the image in</p> <code>'cv2'</code> <p>Returns:</p> Type Description <code>Any</code> <p>The image in the specified format</p> Source code in <code>autodistill/helpers.py</code> <pre><code>def load_image(\n    image: Any,\n    return_format=\"cv2\",\n) -&gt; Any:\n\"\"\"\n    Load an image from a file path, URI, PIL image, or numpy array.\n\n    This function is for use by Autodistill modules. You don't need to use it directly.\n\n    Args:\n        image: The image to load\n        return_format: The format to return the image in\n\n    Returns:\n        The image in the specified format\n    \"\"\"\n    if return_format not in ACCEPTED_RETURN_FORMATS:\n        raise ValueError(f\"return_format must be one of {ACCEPTED_RETURN_FORMATS}\")\n\n    if isinstance(image, Image.Image) and return_format == \"PIL\":\n        return image\n    elif isinstance(image, Image.Image) and return_format == \"cv2\":\n        # channels need to be reversed for cv2\n        return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n    elif isinstance(image, Image.Image) and return_format == \"numpy\":\n        return np.array(image)\n\n    if isinstance(image, np.ndarray) and return_format == \"PIL\":\n        return Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    elif isinstance(image, np.ndarray) and return_format == \"cv2\":\n        return image\n    elif isinstance(image, np.ndarray) and return_format == \"numpy\":\n        return image\n\n    if isinstance(image, str) and image.startswith(\"http\"):\n        if return_format == \"PIL\":\n            response = requests.get(image)\n            return Image.open(BytesIO(response.content))\n        elif return_format == \"cv2\" or return_format == \"numpy\":\n            response = requests.get(image)\n            pil_image = Image.open(BytesIO(response.content))\n            return np.array(pil_image)\n    elif os.path.isfile(image):\n        if return_format == \"PIL\":\n            return Image.open(image)\n        elif return_format == \"cv2\":\n            # channels need to be reversed for cv2\n            return cv2.cvtColor(np.array(Image.open(image)), cv2.COLOR_RGB2BGR)\n        elif return_format == \"numpy\":\n            pil_image = Image.open(image)\n            return np.array(pil_image)\n    else:\n        raise ValueError(f\"{image} is not a valid file path or URI\")\n</code></pre>"},{"location":"reference/utilities/#split-video-frames","title":"Split Video Frames","text":"<p>Split a video into frames and save them to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The path to the video</p> required <code>output_dir</code> <code>str</code> <p>The directory to save the frames to</p> required <code>stride</code> <code>int</code> <p>The stride to use when splitting the video into frames</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>autodistill/helpers.py</code> <pre><code>def split_video_frames(video_path: str, output_dir: str, stride: int) -&gt; None:\n\"\"\"\n    Split a video into frames and save them to a directory.\n\n    Args:\n        video_path: The path to the video\n        output_dir: The directory to save the frames to\n        stride: The stride to use when splitting the video into frames\n\n    Returns:\n        None\n    \"\"\"\n    video_paths = sv.list_files_with_extensions(\n        directory=video_path, extensions=[\"mov\", \"mp4\", \"MOV\", \"MP4\"]\n    )\n\n    for name in tqdm(video_paths):\n        image_name_pattern = name + \"-{:05d}.jpg\"\n        with sv.ImageSink(\n            target_dir_path=output_dir, image_name_pattern=image_name_pattern\n        ) as sink:\n            for image in sv.get_video_frames_generator(\n                source_path=str(video_path), stride=stride\n            ):\n                sink.save_image(image=image)\n</code></pre>"},{"location":"reference/base-models/classification/","title":"Classification","text":"<p>             Bases: <code>BaseModel</code></p> <p>Use a foundation classification model to auto-label data.</p> Source code in <code>autodistill/classification/classification_base_model.py</code> <pre><code>@dataclass\nclass ClassificationBaseModel(BaseModel):\n\"\"\"\n    Use a foundation classification model to auto-label data.\n    \"\"\"\n\n    ontology: CaptionOntology\n\n    @abstractmethod\n    def predict(self, input: str) -&gt; sv.Classifications:\n\"\"\"\n        Run inference on the model.\n        \"\"\"\n        pass\n\n    def label(\n        self,\n        input_folder: str,\n        extension: str = \".jpg\",\n        output_folder: str | None = None,\n    ) -&gt; sv.ClassificationDataset:\n\"\"\"\n        Label a dataset and save it in a classification folder structure.\n        \"\"\"\n        if output_folder is None:\n            output_folder = input_folder + \"_labeled\"\n\n        os.makedirs(output_folder, exist_ok=True)\n\n        image_paths = glob.glob(input_folder + \"/*\" + extension)\n        detections_map = {}\n\n        progress_bar = tqdm(image_paths, desc=\"Labeling images\")\n        for f_path in progress_bar:\n            progress_bar.set_description(desc=f\"Labeling {f_path}\", refresh=True)\n\n            detections = self.predict(f_path)\n            detections_map[f_path] = detections\n\n        dataset = sv.ClassificationDataset(\n            self.ontology.classes(), image_paths, detections_map\n        )\n\n        train_cs, test_cs = dataset.split(\n            split_ratio=0.7, random_state=None, shuffle=True\n        )\n        test_cs, valid_cs = test_cs.split(\n            split_ratio=0.5, random_state=None, shuffle=True\n        )\n\n        train_cs.as_folder_structure(root_directory_path=output_folder + \"/train\")\n\n        test_cs.as_folder_structure(root_directory_path=output_folder + \"/test\")\n\n        valid_cs.as_folder_structure(root_directory_path=output_folder + \"/valid\")\n\n        print(\"Labeled dataset created - ready for distillation.\")\n        return dataset\n</code></pre>"},{"location":"reference/base-models/classification/#autodistill.classification.classification_base_model.ClassificationBaseModel.label","title":"<code>label(input_folder, extension='.jpg', output_folder=None)</code>","text":"<p>Label a dataset and save it in a classification folder structure.</p> Source code in <code>autodistill/classification/classification_base_model.py</code> <pre><code>def label(\n    self,\n    input_folder: str,\n    extension: str = \".jpg\",\n    output_folder: str | None = None,\n) -&gt; sv.ClassificationDataset:\n\"\"\"\n    Label a dataset and save it in a classification folder structure.\n    \"\"\"\n    if output_folder is None:\n        output_folder = input_folder + \"_labeled\"\n\n    os.makedirs(output_folder, exist_ok=True)\n\n    image_paths = glob.glob(input_folder + \"/*\" + extension)\n    detections_map = {}\n\n    progress_bar = tqdm(image_paths, desc=\"Labeling images\")\n    for f_path in progress_bar:\n        progress_bar.set_description(desc=f\"Labeling {f_path}\", refresh=True)\n\n        detections = self.predict(f_path)\n        detections_map[f_path] = detections\n\n    dataset = sv.ClassificationDataset(\n        self.ontology.classes(), image_paths, detections_map\n    )\n\n    train_cs, test_cs = dataset.split(\n        split_ratio=0.7, random_state=None, shuffle=True\n    )\n    test_cs, valid_cs = test_cs.split(\n        split_ratio=0.5, random_state=None, shuffle=True\n    )\n\n    train_cs.as_folder_structure(root_directory_path=output_folder + \"/train\")\n\n    test_cs.as_folder_structure(root_directory_path=output_folder + \"/test\")\n\n    valid_cs.as_folder_structure(root_directory_path=output_folder + \"/valid\")\n\n    print(\"Labeled dataset created - ready for distillation.\")\n    return dataset\n</code></pre>"},{"location":"reference/base-models/classification/#autodistill.classification.classification_base_model.ClassificationBaseModel.predict","title":"<code>predict(input)</code>  <code>abstractmethod</code>","text":"<p>Run inference on the model.</p> Source code in <code>autodistill/classification/classification_base_model.py</code> <pre><code>@abstractmethod\ndef predict(self, input: str) -&gt; sv.Classifications:\n\"\"\"\n    Run inference on the model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/base-models/composed/","title":"Composed Model","text":"<p>             Bases: <code>DetectionBaseModel</code></p> <p>Run inference with a detection model then run inference with a classification model on the detected regions.</p> Source code in <code>autodistill/core/composed_detection_model.py</code> <pre><code>class ComposedDetectionModel(DetectionBaseModel):\n\"\"\"\n    Run inference with a detection model then run inference with a classification model on the detected regions.\n    \"\"\"\n\n    def __init__(\n        self,\n        detection_model,\n        classification_model,\n        set_of_marks=None,\n        set_of_marks_annotator=DEFAULT_LABEL_ANNOTATOR,\n    ):\n        self.detection_model = detection_model\n        self.classification_model = classification_model\n        self.set_of_marks = set_of_marks\n        self.set_of_marks_annotator = set_of_marks_annotator\n        self.ontology = self.classification_model.ontology\n\n    def predict(self, image: str) -&gt; sv.Detections:\n\"\"\"\n        Run inference with a detection model then run inference with a classification model on the detected regions.\n\n        Args:\n            image: The image to run inference on\n            annotator: The annotator to use to annotate the image\n\n        Returns:\n            detections (sv.Detections)\n        \"\"\"\n        opened_image = Image.open(image)\n\n        detections = self.detection_model.predict(image)\n\n        if self.set_of_marks is not None:\n            labels = [f\"{num}\" for num in range(len(detections.xyxy))]\n\n            opened_image = np.array(opened_image)\n\n            annotated_frame = self.set_of_marks_annotator.annotate(\n                scene=opened_image, labels=labels, detections=detections\n            )\n\n            opened_image = Image.fromarray(annotated_frame)\n\n            opened_image.save(\"temp.jpeg\")\n\n            if not hasattr(self.classification_model, \"set_of_marks\"):\n                raise Exception(\n                    f\"The set classification model does not have a set_of_marks method. Supported models: {SET_OF_MARKS_SUPPORTED_MODELS}\"\n                )\n\n            result = self.classification_model.set_of_marks(\n                input=image, masked_input=\"temp.jpeg\", classes=labels, masks=detections\n            )\n\n            return detections\n\n        for pred_idx, bbox in enumerate(detections.xyxy):\n            # extract region from image\n            region = opened_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n\n            # save as tempfile\n            region.save(\"temp.jpeg\")\n\n            result = self.classification_model.predict(\"temp.jpeg\")\n\n            if len(result.class_id) == 0:\n                continue\n\n            result = result.get_top_k(1)[0][0]\n\n            detections.class_id[pred_idx] = result\n\n        return detections\n</code></pre>"},{"location":"reference/base-models/composed/#autodistill.core.composed_detection_model.ComposedDetectionModel.predict","title":"<code>predict(image)</code>","text":"<p>Run inference with a detection model then run inference with a classification model on the detected regions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The image to run inference on</p> required <code>annotator</code> <p>The annotator to use to annotate the image</p> required <p>Returns:</p> Type Description <code>sv.Detections</code> <p>detections (sv.Detections)</p> Source code in <code>autodistill/core/composed_detection_model.py</code> <pre><code>def predict(self, image: str) -&gt; sv.Detections:\n\"\"\"\n    Run inference with a detection model then run inference with a classification model on the detected regions.\n\n    Args:\n        image: The image to run inference on\n        annotator: The annotator to use to annotate the image\n\n    Returns:\n        detections (sv.Detections)\n    \"\"\"\n    opened_image = Image.open(image)\n\n    detections = self.detection_model.predict(image)\n\n    if self.set_of_marks is not None:\n        labels = [f\"{num}\" for num in range(len(detections.xyxy))]\n\n        opened_image = np.array(opened_image)\n\n        annotated_frame = self.set_of_marks_annotator.annotate(\n            scene=opened_image, labels=labels, detections=detections\n        )\n\n        opened_image = Image.fromarray(annotated_frame)\n\n        opened_image.save(\"temp.jpeg\")\n\n        if not hasattr(self.classification_model, \"set_of_marks\"):\n            raise Exception(\n                f\"The set classification model does not have a set_of_marks method. Supported models: {SET_OF_MARKS_SUPPORTED_MODELS}\"\n            )\n\n        result = self.classification_model.set_of_marks(\n            input=image, masked_input=\"temp.jpeg\", classes=labels, masks=detections\n        )\n\n        return detections\n\n    for pred_idx, bbox in enumerate(detections.xyxy):\n        # extract region from image\n        region = opened_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n\n        # save as tempfile\n        region.save(\"temp.jpeg\")\n\n        result = self.classification_model.predict(\"temp.jpeg\")\n\n        if len(result.class_id) == 0:\n            continue\n\n        result = result.get_top_k(1)[0][0]\n\n        detections.class_id[pred_idx] = result\n\n    return detections\n</code></pre>"},{"location":"reference/base-models/detection/","title":"Detection","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>autodistill/detection/detection_base_model.py</code> <pre><code>@dataclass\nclass DetectionBaseModel(BaseModel):\n    ontology: DetectionOntology\n\n    @abstractmethod\n    def predict(self, input: str | np.ndarray | Image.Image) -&gt; sv.Detections:\n        pass\n\n    def sahi_predict(self, input: str | np.ndarray | Image.Image) -&gt; sv.Detections:\n        slicer = sv.InferenceSlicer(callback=self.predict)\n\n        return slicer(load_image(input, return_format=\"cv2\"))\n\n    def _record_confidence_in_files(\n        self,\n        annotations_directory_path: str,\n        image_names: List[str],\n        annotations: Dict[str, sv.Detections],\n    ) -&gt; None:\n        Path(annotations_directory_path).mkdir(parents=True, exist_ok=True)\n        for image_name in image_names:\n            detections = annotations[image_name]\n            yolo_annotations_name, _ = os.path.splitext(image_name)\n            confidence_path = os.path.join(\n                annotations_directory_path,\n                \"confidence-\" + yolo_annotations_name + \".txt\",\n            )\n            if detections.confidence is None:\n                raise ValueError(\"Expected detections to have confidence values.\")\n            confidence_list = [str(x) for x in detections.confidence.tolist()]\n            save_text_file(lines=confidence_list, file_path=confidence_path)\n            print(\"Saved confidence file: \" + confidence_path)\n\n    def label(\n        self,\n        input_folder: str,\n        extension: str = \".jpg\",\n        output_folder: str | None = None,\n        human_in_the_loop: bool = False,\n        roboflow_project: str | None = None,\n        roboflow_tags: list[str] = [\"autodistill\"],\n        sahi: bool = False,\n        record_confidence: bool = False,\n        nms_settings: NmsSetting = NmsSetting.NONE,\n    ) -&gt; sv.DetectionDataset:\n\"\"\"\n        Label a dataset with the model.\n        \"\"\"\n        if output_folder is None:\n            output_folder = input_folder + \"_labeled\"\n\n        os.makedirs(output_folder, exist_ok=True)\n\n        image_paths = glob.glob(input_folder + \"/*\" + extension)\n        detections_map = {}\n\n        if sahi:\n            slicer = sv.InferenceSlicer(callback=self.predict)\n\n        progress_bar = tqdm(image_paths, desc=\"Labeling images\")\n        for f_path in progress_bar:\n            progress_bar.set_description(desc=f\"Labeling {f_path}\", refresh=True)\n\n            image = cv2.imread(f_path)\n            if sahi:\n                detections = slicer(image)\n            else:\n                detections = self.predict(image)\n\n            if nms_settings == NmsSetting.CLASS_SPECIFIC:\n                detections = detections.with_nms()\n            if nms_settings == NmsSetting.CLASS_AGNOSTIC:\n                detections = detections.with_nms(class_agnostic=True)\n\n            detections_map[f_path] = detections\n\n        dataset = sv.DetectionDataset(\n            self.ontology.classes(), image_paths, detections_map\n        )\n\n        dataset.as_yolo(\n            output_folder + \"/images\",\n            output_folder + \"/annotations\",\n            min_image_area_percentage=0.01,\n            data_yaml_path=output_folder + \"/data.yaml\",\n        )\n\n        if record_confidence:\n            image_names = [os.path.basename(f_path) for f_path in image_paths]\n            self._record_confidence_in_files(\n                output_folder + \"/annotations\", image_names, detections_map\n            )\n        split_data(output_folder, record_confidence=record_confidence)\n\n        if human_in_the_loop:\n            roboflow.login()\n\n            rf = roboflow.Roboflow()\n\n            workspace = rf.workspace()\n\n            workspace.upload_dataset(output_folder, project_name=roboflow_project)\n\n        print(\"Labeled dataset created - ready for distillation.\")\n        return dataset\n</code></pre>"},{"location":"reference/base-models/detection/#autodistill.detection.detection_base_model.DetectionBaseModel.label","title":"<code>label(input_folder, extension='.jpg', output_folder=None, human_in_the_loop=False, roboflow_project=None, roboflow_tags=['autodistill'], sahi=False, record_confidence=False, nms_settings=NmsSetting.NONE)</code>","text":"<p>Label a dataset with the model.</p> Source code in <code>autodistill/detection/detection_base_model.py</code> <pre><code>def label(\n    self,\n    input_folder: str,\n    extension: str = \".jpg\",\n    output_folder: str | None = None,\n    human_in_the_loop: bool = False,\n    roboflow_project: str | None = None,\n    roboflow_tags: list[str] = [\"autodistill\"],\n    sahi: bool = False,\n    record_confidence: bool = False,\n    nms_settings: NmsSetting = NmsSetting.NONE,\n) -&gt; sv.DetectionDataset:\n\"\"\"\n    Label a dataset with the model.\n    \"\"\"\n    if output_folder is None:\n        output_folder = input_folder + \"_labeled\"\n\n    os.makedirs(output_folder, exist_ok=True)\n\n    image_paths = glob.glob(input_folder + \"/*\" + extension)\n    detections_map = {}\n\n    if sahi:\n        slicer = sv.InferenceSlicer(callback=self.predict)\n\n    progress_bar = tqdm(image_paths, desc=\"Labeling images\")\n    for f_path in progress_bar:\n        progress_bar.set_description(desc=f\"Labeling {f_path}\", refresh=True)\n\n        image = cv2.imread(f_path)\n        if sahi:\n            detections = slicer(image)\n        else:\n            detections = self.predict(image)\n\n        if nms_settings == NmsSetting.CLASS_SPECIFIC:\n            detections = detections.with_nms()\n        if nms_settings == NmsSetting.CLASS_AGNOSTIC:\n            detections = detections.with_nms(class_agnostic=True)\n\n        detections_map[f_path] = detections\n\n    dataset = sv.DetectionDataset(\n        self.ontology.classes(), image_paths, detections_map\n    )\n\n    dataset.as_yolo(\n        output_folder + \"/images\",\n        output_folder + \"/annotations\",\n        min_image_area_percentage=0.01,\n        data_yaml_path=output_folder + \"/data.yaml\",\n    )\n\n    if record_confidence:\n        image_names = [os.path.basename(f_path) for f_path in image_paths]\n        self._record_confidence_in_files(\n            output_folder + \"/annotations\", image_names, detections_map\n        )\n    split_data(output_folder, record_confidence=record_confidence)\n\n    if human_in_the_loop:\n        roboflow.login()\n\n        rf = roboflow.Roboflow()\n\n        workspace = rf.workspace()\n\n        workspace.upload_dataset(output_folder, project_name=roboflow_project)\n\n    print(\"Labeled dataset created - ready for distillation.\")\n    return dataset\n</code></pre>"},{"location":"reference/base-models/embedding/","title":"Embedding Model","text":"<p>             Bases: <code>ABC</code></p> <p>Use an embedding model to calculate embeddings for use in classification.</p> Source code in <code>autodistill/core/embedding_model.py</code> <pre><code>@dataclass\nclass EmbeddingModel(ABC):\n\"\"\"\n    Use an embedding model to calculate embeddings for use in classification.\n    \"\"\"\n\n    ontology: Ontology\n\n    def set_ontology(self, ontology: Ontology):\n\"\"\"\n        Set the ontology for the model.\n        \"\"\"\n        self.ontology = ontology\n\n    @abstractmethod\n    def embed_image(self, input: Any) -&gt; np.ndarray:\n\"\"\"\n        Calculate an image embedding for an image.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def embed_text(self, input: Any) -&gt; np.ndarray:\n\"\"\"\n        Calculate a text embedding for an image.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/base-models/embedding/#autodistill.core.embedding_model.EmbeddingModel.embed_image","title":"<code>embed_image(input)</code>  <code>abstractmethod</code>","text":"<p>Calculate an image embedding for an image.</p> Source code in <code>autodistill/core/embedding_model.py</code> <pre><code>@abstractmethod\ndef embed_image(self, input: Any) -&gt; np.ndarray:\n\"\"\"\n    Calculate an image embedding for an image.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/base-models/embedding/#autodistill.core.embedding_model.EmbeddingModel.embed_text","title":"<code>embed_text(input)</code>  <code>abstractmethod</code>","text":"<p>Calculate a text embedding for an image.</p> Source code in <code>autodistill/core/embedding_model.py</code> <pre><code>@abstractmethod\ndef embed_text(self, input: Any) -&gt; np.ndarray:\n\"\"\"\n    Calculate a text embedding for an image.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/base-models/embedding/#autodistill.core.embedding_model.EmbeddingModel.set_ontology","title":"<code>set_ontology(ontology)</code>","text":"<p>Set the ontology for the model.</p> Source code in <code>autodistill/core/embedding_model.py</code> <pre><code>def set_ontology(self, ontology: Ontology):\n\"\"\"\n    Set the ontology for the model.\n    \"\"\"\n    self.ontology = ontology\n</code></pre>"},{"location":"reference/ontologies/caption-ontology/","title":"Caption Ontology","text":"<p>             Bases: <code>DetectionOntology</code></p> Source code in <code>autodistill/detection/caption_ontology.py</code> <pre><code>@dataclass\nclass CaptionOntology(DetectionOntology):\n    promptMap: List[Tuple[str, str]]\n\n    def __init__(self, ontology: Dict[str, str]):\n        self.promptMap = [(k, v) for k, v in ontology.items()]\n\n        if len(self.promptMap) == 0:\n            raise ValueError(\"Ontology is empty\")\n\n    def prompts(self) -&gt; List[str]:\n        return super().prompts()\n\n    def classToPrompt(self, cls: str) -&gt; str:\n        return super().classToPrompt(cls)\n</code></pre>"},{"location":"reference/ontologies/embedding-ontology/","title":"Embedding Ontology","text":"<p>             Bases: <code>Ontology</code></p> Source code in <code>autodistill/core/embedding_ontology.py</code> <pre><code>@dataclass\nclass EmbeddingOntology(Ontology):\n    embeddingMap: Dict[str, np.ndarray]\n\n    def __init__(self, embeddingMap, cluster=1):\n        self.embeddingMap = embeddingMap\n\n    @classmethod\n    def process(self, model: EmbeddingModel):\n        pass\n\n    def prompts(self) -&gt; List[np.ndarray]:\n        return [prompt for prompt, _ in self.embeddingMap]\n\n    def classes(self) -&gt; List[str]:\n        return [cls for _, cls in self.embeddingMap]\n</code></pre>"},{"location":"reference/target-models/classification/","title":"Classification","text":"<p>             Bases: <code>TargetModel</code></p> Source code in <code>autodistill/classification/classification_target_model.py</code> <pre><code>class ClassificationTargetModel(TargetModel):\n    @abstractmethod\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def predict(self, input: str, confidence: float = 0.5) -&gt; sv.Classifications:\n        pass\n\n    @abstractmethod\n    def train(self):\n        pass\n</code></pre>"},{"location":"reference/target-models/detection/","title":"Detection","text":"<p>             Bases: <code>TargetModel</code></p> Source code in <code>autodistill/detection/detection_target_model.py</code> <pre><code>class DetectionTargetModel(TargetModel):\n    @abstractmethod\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def predict(self, input: str, confidence: float = 0.5) -&gt; sv.Detections:\n        pass\n\n    @abstractmethod\n    def train(self):\n        pass\n</code></pre>"},{"location":"target_models/detr/","title":"DETR","text":"<p>Object Detection Target Model</p>"},{"location":"target_models/detr/#what-is-detr","title":"What is DETR?","text":"<p>This repository contains the code supporting the DETR base model for use with Autodistill.</p> <p>DETR is a transformer-based computer vision model you can use for object detection. Autodistill supports training a model using the Meta Research Resnet 50 checkpoint.</p> <p>Read the full Autodistill documentation.</p> <p>Read the DETR Autodistill documentation.</p>"},{"location":"target_models/detr/#installation","title":"Installation","text":"<p>To use DETR with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-detr\n</code></pre>"},{"location":"target_models/detr/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_detr import DETR\n\n# load the model\ntarget_model = DETR()\n\n# train for 10 epochs\ntarget_model.train(\"./roads\", epochs=10)\n\n# run inference on an image\ntarget_model.predict(\"./roads/valid/-3-_jpg.rf.bee113a09b22282980c289842aedfc4a.jpg\")\n</code></pre>"},{"location":"target_models/detr/#license","title":"License","text":"<p>This project is licensed under an Apache 2.0 license. See the Hugging Face model card for the DETR Resnet 50 model for more information on the model license.</p>"},{"location":"target_models/dinov2/","title":"DINOv2","text":"<p>Classification Base Model</p>"},{"location":"target_models/dinov2/#what-is-dinov2","title":"What is DINOv2?","text":"<p>This repository contains the code supporting the DINOv2 base model for use with Autodistill.</p> <p>DINOv2, developed by Meta Research, is a self-supervised training method for computer vision models. This library uses DINOv2 image embeddings with SVM to build a classification model.</p>"},{"location":"target_models/dinov2/#installation","title":"Installation","text":"<p>To use DINOv2 with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-dinov2\n</code></pre>"},{"location":"target_models/dinov2/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_dinov2 import DINOv2\n\ntarget_model = DINOv2(None)\n\n# train a model\n# specify the directory where your annotations (in multiclass classification folder format)\n# DINOv2 embeddings are saved in a file called \"embeddings.json\" the folder in which you are working\n# with the structure {filename: embedding}\ntarget_model.train(\"./context_images_labeled\")\n\n# get class list\n# print(target_model.ontology.classes())\n\n# run inference on the new model\npred = target_model.predict(\"./context_images_labeled/train/images/dog-7.jpg\")\n\nprint(pred)\n</code></pre>"},{"location":"target_models/distilbert/","title":"DistilBERT","text":"<p>Text Classification Target Model</p>"},{"location":"target_models/distilbert/#what-is-distilbert","title":"What is DistilBERT?","text":"<p>DistilBERT is a languae model architecture commonly used in training sentence classification models. You can use <code>autodistill</code> to train a DistilBERT model that classifies text.</p>"},{"location":"target_models/distilbert/#installation","title":"Installation","text":"<p>To use the DistilBERT target model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-distilbert-text\n</code></pre>"},{"location":"target_models/distilbert/#quickstart","title":"Quickstart","text":"<p>The DistilBERT module takes in <code>.jsonl</code> files and trains a text classification model.</p> <p>Each record in the JSONL file should have an entry called <code>text</code> that contains the text to be classified. The <code>label</code> entry should contain the ground truth label for the text. This format is returned by Autodistill base text classification models like the GPTClassifier.</p> <p>Here is an example entry of a record used to train a research paper subject classifier:</p> <pre><code>{\"title\": \"CC-GPX: Extracting High-Quality Annotated Geospatial Data from Common Crawl\", \"content\": \"arXiv:2405.11039v1 Announce Type: new \\nAbstract: The Common Crawl (CC) corpus....\", \"classification\": \"natural language processing\"}\n</code></pre> <pre><code>from autodistill_distilbert import DistilBERT\n\ntarget_model = DistilBERT()\n\n# train a model\ntarget_model.train(\"./data.jsonl\", epochs=200)\n\n# run inference on the new model\npred = target_model.predict(\"Geospatial data.\", conf=0.01)\n\nprint(pred)\n# geospatial\n</code></pre>"},{"location":"target_models/florence2/","title":"Florence-2","text":"<p>Object Detection Base Model Target Model</p>"},{"location":"target_models/florence2/#florence-2","title":"Florence-2","text":"<p>Florence 2, introduced in the paper Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks is a multimodal vision model.</p> <p>You can use Florence 2 to generate object detection annotations for use in training smaller object detection models with Autodistill.</p> <p>Read the full Autodistill documentation.</p> <p>Read the Florence 2 Autodistill documentation.</p>"},{"location":"target_models/florence2/#installation","title":"Installation","text":"<p>To use Florence 2 with Autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-florence-2\n</code></pre>"},{"location":"target_models/florence2/#quickstart-inference-from-base-weights","title":"Quickstart (Inference from Base Weights)","text":"<pre><code>from autodistill_florence_2 import Florence2\nfrom autodistill.detection import DetectionOntology\nfrom PIL import Image\n\n# define an ontology to map class names to our Florence 2 prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = Florence2(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\n\nimage = Image.open(\"image.jpeg\")\nresult = base_model.predict('image.jpeg')\n\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nannotated_frame = bounding_box_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\nsv.plot_image(image=annotated_frame, size=(16, 16))\n\n# label a dataset\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"target_models/florence2/#quickstart-fine-tune","title":"Quickstart (Fine-Tune)","text":"<pre><code>from autodistill_florence_2 import Florence2Trainer\n\nmodel = Florence2Trainer(\"dataset\")\nmodel.train(dataset.location, epochs=10)\n</code></pre>"},{"location":"target_models/florence2/#license","title":"License","text":"<p>This project is licensed under an MIT license. See the Florence 2 license for more information about the Florence 2 model license.</p>"},{"location":"target_models/florence2/#contributing","title":"\ud83c\udfc6 Contributing","text":"<p>We love your input! Please see the core Autodistill contributing guide to get started. Thank you \ud83d\ude4f to all our contributors!</p>"},{"location":"target_models/setfit/","title":"SetFit","text":"<p>Text Classification Target Model</p>"},{"location":"target_models/setfit/#what-is-setfit","title":"What is SetFit?","text":"<p>SetFit is a framework for fine-tuning Sentence Transformer models with a few examples of each class on which you want to train. SetFit is developed by Hugging Face.</p>"},{"location":"target_models/setfit/#installation","title":"Installation","text":"<p>To use the SetFit target model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-setfit\n</code></pre>"},{"location":"target_models/setfit/#quickstart","title":"Quickstart","text":"<p>The SetFit module takes in <code>.jsonl</code> files and trains a text classification model.</p> <p>Each record in the JSONL file should have an entry called <code>text</code> that contains the text to be classified. The <code>label</code> entry should contain the ground truth label for the text. This format is returned by Autodistill base text classification models like the GPTClassifier.</p> <p>Here is an example entry of a record used to train a research paper subject classifier:</p> <pre><code>{\"title\": \"CC-GPX: Extracting High-Quality Annotated Geospatial Data from Common Crawl\", \"content\": \"arXiv:2405.11039v1 Announce Type: new \\nAbstract: The Common Crawl (CC) corpus....\", \"classification\": \"natural language processing\"}\n</code></pre> <pre><code>from autodistill_setfit import SetFitModel\n\ntarget_model = SetFitModel()\n\n# train a model\ntarget_model.train(\"./data.jsonl\", output=\"model\", epochs=5)\n\ntarget_model = SetFitModel(\"model\")\n\n# run inference on the new model\npred = target_model.predict(\"Geospatial data.\")\n\nprint(pred)\n# geospatial\n</code></pre>"},{"location":"target_models/vit/","title":"ViT","text":"<p>Classification Target Model</p>"},{"location":"target_models/vit/#what-is-vit","title":"What is ViT?","text":"<p>ViT is a classification model pre-trained on ImageNet-21k, developed by Google. You can train ViT classification models using Autodistill.</p> <p>Read the full Autodistill documentation.</p> <p>Read the ViT Autodistill documentation.</p>"},{"location":"target_models/vit/#installation","title":"Installation","text":"<p>To use the ViT target model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-vit\n</code></pre>"},{"location":"target_models/vit/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_vit import ViT\n\ntarget_model = ViT()\n\n# train a model from a classification folder structure\ntarget_model.train(\"./context_images_labeled/\", epochs=200)\n\n# run inference on the new model\npred = target_model.predict(\"./context_images_labeled/train/images/dog-7.jpg\", conf=0.01)\n</code></pre>"},{"location":"target_models/yolonas/","title":"YOLO-NAS","text":"<p>Object Detection Target Model</p>"},{"location":"target_models/yolonas/#what-is-yolo-nas","title":"What is YOLO-NAS?","text":"<p>YOLO-NAS is an object detection model developed by Deci AI.</p> <p>You can use <code>autodistill</code> to train a YOLO-NAS object detection model on a dataset of labelled images generated by the base models that <code>autodistill</code> supports.</p> <p>Read the full Autodistill documentation.</p> <p>Read the YOLO-NAS Autodistill documentation.</p>"},{"location":"target_models/yolonas/#installation","title":"Installation","text":"<p>To use the YOLO-NAS target model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-yolonas\n</code></pre>"},{"location":"target_models/yolonas/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_yolonas import YOLONAS\n\ntarget_model = YOLONAS()\n\n# train a model\n# specify the directory where your annotations (in YOLO format) are stored\ntarget_model.train(\"./context_images_labeled\", epochs=20)\n\n# run inference on the new model\npred = target_model.predict(\"./context_images_labeled/train/images/dog-7.jpg\", confidence=0.01)\n</code></pre>"},{"location":"target_models/yolonas/#license","title":"License","text":"<p>The YOLO-NAS model is licensed under the YOLO-NAS License.</p>"},{"location":"target_models/yolov5/","title":"YOLOv5","text":"<p>Object Detection Target Model</p>"},{"location":"target_models/yolov5/#what-is-yolov5","title":"What is YOLOv5?","text":"<p>YOLOv5 is an open-source computer vision model by Ultralytics, the creators of YOLOv5. You can use <code>autodistill</code> to train a YOLOv5 object detection model on a dataset of labelled images generated by the base models that <code>autodistill</code> supports.</p> <p>View our YOLOv5 Instance Segmentation page for information on how to train instance segmentation models.</p> <p>Read the full Autodistill documentation.</p> <p>Read the YOLOv5 Autodistill documentation.</p>"},{"location":"target_models/yolov5/#installation","title":"Installation","text":"<p>To use the YOLOv5 target model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-yolov5\n</code></pre>"},{"location":"target_models/yolov5/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_YOLOv5 import YOLOv5\n\ntarget_model = YOLOv5(\"YOLOv5n.pt\")\n\n# train a model\ntarget_model.train(\"./context_images_labeled/data.yaml\", epochs=200)\n\n# run inference on the new model\npred = target_model.predict(\"./context_images_labeled/train/images/dog-7.jpg\", conf=0.01)\n</code></pre>"},{"location":"target_models/yolov5/#license","title":"License","text":"<p>The code in this repository is licensed under an AGPL 3.0 license.</p>"},{"location":"target_models/yolov8/","title":"YOLOv8","text":"<p>Object Detection Segmentation Target Model</p>"},{"location":"target_models/yolov8/#what-is-yolov8","title":"What is YOLOv8?","text":"<p>Ultralytics YOLOv8 is a Convolutional Neural Network (CNN) that supports realtime object detection, instance segmentation, and other tasks. It can be deployed to a variety of edge devices.</p> <p>Read the full Autodistill documentation.</p> <p>Read the YOLOv8 Autodistill documentation.</p>"},{"location":"target_models/yolov8/#installation","title":"Installation","text":"<p>To use the YOLOv8 Target Model, simply install it along with a Base Model supporting the <code>detection</code> task:</p> <pre><code>pip3 install autodistill-grounded-sam autodistill-yolov8\n</code></pre> <p>You can find a full list of <code>detection</code> Base Models on the main autodistill repo.</p>"},{"location":"target_models/yolov8/#quickstart-train-a-yolov8-model","title":"Quickstart (Train a YOLOv8 Model)","text":"<pre><code>from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_yolov8 import YOLOv8\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\nbase_model = GroundedSAM(ontology=CaptionOntology({\"shipping container\": \"container\"}))\n\n# label all images in a folder called `context_images`\nbase_model.label(\n  input_folder=\"./images\",\n  output_folder=\"./dataset\"\n)\n\ntarget_model = YOLOv8(\"yolov8n.pt\")\ntarget_model.train(\"./dataset/data.yaml\", epochs=200)\n\n# run inference on the new model\npred = target_model.predict(\"./dataset/valid/your-image.jpg\", confidence=0.5)\nprint(pred)\n\n# optional: upload your model to Roboflow for deployment\nfrom roboflow import Roboflow\n\nrf = Roboflow(api_key=\"API_KEY\")\nproject = rf.workspace().project(\"PROJECT_ID\")\nproject.version(DATASET_VERSION).deploy(model_type=\"yolov8\", model_path=f\"./runs/detect/train/\")\n</code></pre>"},{"location":"target_models/yolov8/#quickstart-use-a-yolov8-model-to-label-data","title":"Quickstart (Use a YOLOv8 Model to Label Data)","text":"<pre><code>from autodistill_yolov8 import YOLOv8Base\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our YOLOv8 classes\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\n\n# replace weights_path with the path to your YOLOv8 weights file\nbase_model = YOLOv8Base(ontology=CaptionOntology({\"car\": \"car\"}), weights_path=\"yolov5s.pt\")\n\n# run inference on a single image\nresults = base_model.predict(\"mercedes.jpeg\")\n\nbase_model.label(\n  input_folder=\"./images\",\n  output_folder=\"./dataset\"\n)\n</code></pre>"},{"location":"target_models/yolov8/#choosing-a-task","title":"Choosing a Task","text":"<p>YOLOv8 supports training both object detection and instance segmentation tasks at various sizes (larger models are slower but can be more accurate). This selection is done in the constructor.</p> <p>For example: <pre><code># initializes a nano-sized instance segmentation model\ntarget_model = YOLOv8(\"yolov8n-seg.pt\")\n</code></pre></p> <p>Available object detection initialization options are:</p> <ul> <li><code>yolov8n.pt</code> - nano (3.2M parameters)</li> <li><code>yolov8s.pt</code> - small (11.2M parameters)</li> <li><code>yolov8m.pt</code> - medium (25.9M parameters)</li> <li><code>yolov8l.pt</code> - large (43.7M parameters)</li> <li><code>yolov8x.pt</code> - extra-large (68.2M parameters)</li> </ul> <p>Available instance segmentation initialization options are:</p> <ul> <li><code>yolov8n-seg.pt</code> - nano (3.4M parameters)</li> <li><code>yolov8s-seg.pt</code> - small (11.8M parameters)</li> <li><code>yolov8m-seg.pt</code> - medium (27.3M parameters)</li> <li><code>yolov8l-seg.pt</code> - large (46.0M parameters)</li> <li><code>yolov8x-seg.pt</code> - extra-large (71.8M parameters)</li> </ul>"},{"location":"target_models/yolov8/#license","title":"License","text":"<p>The code in this repository is licensed under an AGPL 3.0 license.</p>"},{"location":"utilities/combine-models/","title":"Combine Models","text":"<p>You can combine detection, segmentation, and classification models to leverage the strengths of each model.</p> <p>For example, consider a scenario where you want to build a logo detection model that identifies popular logos. You could use a detection model to identify logos (i.e. Grounding DINO), then a classification model to classify between the logos (i.e. Microsoft, Apple, etc.).</p> <p>To combine models, you need to choose:</p> <ol> <li>Either a detection or a segmentation model, and;</li> <li>A classification model.</li> </ol> <p>Let's walk through an example of using a combination of Grounding DINO and SAM (GroundedSAM), and CLIP for logo classification.</p> <pre><code>from autodistill_clip import CLIP\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_grounded_sam import GroundedSAM\nimport supervision as sv\n\nfrom autodistill.core.custom_detection_model import CustomDetectionModel\nimport cv2\n\nclasses = [\"McDonalds\", \"Burger King\"]\n\n\nSAMCLIP = CustomDetectionModel(\n    detection_model=GroundedSAM(\n        CaptionOntology({\"logo\": \"logo\"})\n    ),\n    classification_model=CLIP(\n        CaptionOntology({k: k for k in classes})\n    )\n)\n\nIMAGE = \"logo.jpg\"\n\nresults = SAMCLIP.predict(IMAGE)\n\nimage = cv2.imread(IMAGE)\n\nannotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    f\"{classes[class_id]} {confidence:0.2f}\"\n    for _, _, confidence, class_id, _ in results\n]\n\nannotated_frame = annotator.annotate(\n    scene=image.copy(), detections=results\n)\nannotated_frame = label_annotator.annotate(\n    scene=annotated_frame, labels=labels, detections=results\n)\n\nsv.plot_image(annotated_frame, size=(8, 8))\n</code></pre> <p>Here are the results:</p> <p></p>"},{"location":"utilities/combine-models/#see-also","title":"See Also","text":"<ul> <li>Automatically Label Product SKUs with Autodistill : Uses a combination of Grounding DINO and CLIP to label product SKUs.</li> </ul>"},{"location":"utilities/combine-models/#code-reference","title":"Code Reference","text":"<p>             Bases: <code>DetectionBaseModel</code></p> <p>Run inference with a detection model then run inference with a classification model on the detected regions.</p> Source code in <code>autodistill/core/composed_detection_model.py</code> <pre><code>class ComposedDetectionModel(DetectionBaseModel):\n\"\"\"\n    Run inference with a detection model then run inference with a classification model on the detected regions.\n    \"\"\"\n\n    def __init__(\n        self,\n        detection_model,\n        classification_model,\n        set_of_marks=None,\n        set_of_marks_annotator=DEFAULT_LABEL_ANNOTATOR,\n    ):\n        self.detection_model = detection_model\n        self.classification_model = classification_model\n        self.set_of_marks = set_of_marks\n        self.set_of_marks_annotator = set_of_marks_annotator\n        self.ontology = self.classification_model.ontology\n\n    def predict(self, image: str) -&gt; sv.Detections:\n\"\"\"\n        Run inference with a detection model then run inference with a classification model on the detected regions.\n\n        Args:\n            image: The image to run inference on\n            annotator: The annotator to use to annotate the image\n\n        Returns:\n            detections (sv.Detections)\n        \"\"\"\n        opened_image = Image.open(image)\n\n        detections = self.detection_model.predict(image)\n\n        if self.set_of_marks is not None:\n            labels = [f\"{num}\" for num in range(len(detections.xyxy))]\n\n            opened_image = np.array(opened_image)\n\n            annotated_frame = self.set_of_marks_annotator.annotate(\n                scene=opened_image, labels=labels, detections=detections\n            )\n\n            opened_image = Image.fromarray(annotated_frame)\n\n            opened_image.save(\"temp.jpeg\")\n\n            if not hasattr(self.classification_model, \"set_of_marks\"):\n                raise Exception(\n                    f\"The set classification model does not have a set_of_marks method. Supported models: {SET_OF_MARKS_SUPPORTED_MODELS}\"\n                )\n\n            result = self.classification_model.set_of_marks(\n                input=image, masked_input=\"temp.jpeg\", classes=labels, masks=detections\n            )\n\n            return detections\n\n        for pred_idx, bbox in enumerate(detections.xyxy):\n            # extract region from image\n            region = opened_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n\n            # save as tempfile\n            region.save(\"temp.jpeg\")\n\n            result = self.classification_model.predict(\"temp.jpeg\")\n\n            if len(result.class_id) == 0:\n                continue\n\n            result = result.get_top_k(1)[0][0]\n\n            detections.class_id[pred_idx] = result\n\n        return detections\n</code></pre>"},{"location":"utilities/combine-models/#autodistill.core.composed_detection_model.ComposedDetectionModel.predict","title":"<code>predict(image)</code>","text":"<p>Run inference with a detection model then run inference with a classification model on the detected regions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The image to run inference on</p> required <code>annotator</code> <p>The annotator to use to annotate the image</p> required <p>Returns:</p> Type Description <code>sv.Detections</code> <p>detections (sv.Detections)</p> Source code in <code>autodistill/core/composed_detection_model.py</code> <pre><code>def predict(self, image: str) -&gt; sv.Detections:\n\"\"\"\n    Run inference with a detection model then run inference with a classification model on the detected regions.\n\n    Args:\n        image: The image to run inference on\n        annotator: The annotator to use to annotate the image\n\n    Returns:\n        detections (sv.Detections)\n    \"\"\"\n    opened_image = Image.open(image)\n\n    detections = self.detection_model.predict(image)\n\n    if self.set_of_marks is not None:\n        labels = [f\"{num}\" for num in range(len(detections.xyxy))]\n\n        opened_image = np.array(opened_image)\n\n        annotated_frame = self.set_of_marks_annotator.annotate(\n            scene=opened_image, labels=labels, detections=detections\n        )\n\n        opened_image = Image.fromarray(annotated_frame)\n\n        opened_image.save(\"temp.jpeg\")\n\n        if not hasattr(self.classification_model, \"set_of_marks\"):\n            raise Exception(\n                f\"The set classification model does not have a set_of_marks method. Supported models: {SET_OF_MARKS_SUPPORTED_MODELS}\"\n            )\n\n        result = self.classification_model.set_of_marks(\n            input=image, masked_input=\"temp.jpeg\", classes=labels, masks=detections\n        )\n\n        return detections\n\n    for pred_idx, bbox in enumerate(detections.xyxy):\n        # extract region from image\n        region = opened_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n\n        # save as tempfile\n        region.save(\"temp.jpeg\")\n\n        result = self.classification_model.predict(\"temp.jpeg\")\n\n        if len(result.class_id) == 0:\n            continue\n\n        result = result.get_top_k(1)[0][0]\n\n        detections.class_id[pred_idx] = result\n\n    return detections\n</code></pre>"},{"location":"utilities/compare-models/","title":"Compare Models","text":"<p>You can compare two or more models on multiple images using the <code>compare</code> function.</p> <p>This function is ideal if you want to evaluate how different models perform on a single image or multiple images.</p> <p>The following example shows how to compare OWLv2 and Grounding DINO on a single image:</p> <pre><code>from autodistill_grounding_dino import GroundingDINO\nfrom autodistill_owlv2 import OWLv2\n\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import compare\n\nontology = CaptionOntology(\n    {\n        \"solar panel\": \"solar panel\",\n    }\n)\n\nmodels = [\n    GroundingDINO(ontology=ontology),\n    OWLv2(ontology=ontology),\n]\n\nimages = [\n    \"./solar.jpg\"\n]\n\ncompare(\n    models=models,\n    images=images\n)\n</code></pre> <p>Here are the results:</p> <p></p> <p>Above, we can see predictions from Grounding DINO and OWLv2.</p>"},{"location":"utilities/compare-models/#code-reference","title":"Code Reference","text":"<p>Compare the predictions of multiple models on multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list</code> <p>The models to compare</p> required <code>images</code> <code>List[str]</code> <p>The images to compare</p> required <p>Returns:</p> Type Description <p>A grid of images with the predictions of each model on each image.</p> Source code in <code>autodistill/utils.py</code> <pre><code>def compare(models: list, images: List[str]):\n\"\"\"\n    Compare the predictions of multiple models on multiple images.\n\n    Args:\n        models: The models to compare\n        images: The images to compare\n\n    Returns:\n        A grid of images with the predictions of each model on each image.\n    \"\"\"\n    image_results = []\n    model_results = []\n\n    for model in models:\n        # get model class name\n        model_name = model.__class__.__name__\n\n        for image in images:\n            results = model.predict(image)\n\n            image_data = cv2.imread(image)\n\n            image_result = plot(\n                image_data, results, classes=model.ontology.prompts(), raw=True\n            )\n\n            image_results.append(image_result)\n\n            model_results.append(model_name)\n\n    sv.plot_images_grid(\n        image_results,\n        grid_size=(len(models), len(images)),\n        titles=model_results,\n        size=(16, 16),\n    )\n</code></pre>"},{"location":"utilities/nms/","title":"Apply Non-Maximum Suppression (NMS)","text":"<p>You can apply Non-Maximum Suppression (NMS) to predictions from a detection model to remove overlapping bounding boxes.</p> <p>To do so, add <code>.with_nms()</code> to the result of any <code>predict()</code> or <code>predict_sahi()</code> method from an object detection model.</p> <p>Here is an example of running NMS on predictions from a Grounding DINO model:</p> Without NMSWith NMS <pre><code>from autodistill_owlv2 import OWLv2\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\n\nimport cv2\n\nontology = CaptionOntology({\"person\": \"person\"})\n\nbase_model = OWLv2(ontology=ontology)\n\ndetections = base_model.predict(\"./dog.jpeg\")\n\nplot(\n    image=cv2.imread(\"./dog.jpeg\"),\n    detections=detections,\n    classes=base_model.ontology.classes(),\n)\n</code></pre> <p></p> <pre><code>from autodistill_owlv2 import OWLv2\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\n\nimport cv2\n\nontology = CaptionOntology({\"person\": \"person\"})\n\nbase_model = OWLv2(ontology=ontology)\n\ndetections = base_model.predict(\"./dog.jpeg\")\n\nplot(\n    image=cv2.imread(\"./dog.jpeg\"),\n    detections=detections.with_nms(),\n    classes=base_model.ontology.classes(),\n)\n</code></pre> <p></p>"},{"location":"utilities/sahi/","title":"Use SAHI to Detect Objects","text":"<p>Slicing Aided Hyper Inference (SAHI) is a technique that improves the detection rate of small objects in an image. SAHI involves splitting up an image into segments, then runs inference on each segment. Then, the results from each segment are combined into a single result.</p> <p>Because SAHI runs inference on separate segments, it will take longer to run inference on an image with SAHI than without SAHI.</p> <p>You can use SAHI when running inference on a single image with Autodistill, or when using Autodistill to label a folder of images.</p>"},{"location":"utilities/sahi/#use-sahi-in-a-single-prediction","title":"Use SAHI in a Single Prediction","text":"<p>To use SAHI in a single prediction, use the <code>sahi</code> parameter in the <code>predict()</code> method:</p> <pre><code>import cv2\nimport supervision as sv\n\nfrom autodistill_grounding_dino import GroundingDINO\nfrom autodistill.detection import CaptionOntology\n\nbase_model = GroundingDINO(ontology=CaptionOntology({\"person\": \"person\"}))\n\ndetections = base_model.predict_sahi(\"./image.jpg\")\n\nclasses = [\"person\"]\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    f\"{classes[class_id]} {confidence:0.2f}\"\n    for _, _, confidence, class_id, _, _\n    in detections\n]\n\nimage = cv2.imread(\"./image.jpg\")\n\nannotated_frame = box_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\nannotated_frame = label_annotator.annotate(\n    scene=annotated_frame,\n    detections=detections,\n    labels=labels\n)\n\nsv.plot_image(image=annotated_frame, size=(16, 16))\n</code></pre> <p>Here are the results before and after SAHI:</p> Without SAHIWith SAHI <p></p> <p></p> <p>The image processed with SAHI detected more people.</p>"},{"location":"utilities/sahi/#use-sahi-to-label-a-folder-of-images","title":"Use SAHI to Label a Folder of Images","text":"<p>To use SAHI to label a folder of images, use the <code>sahi</code> parameter in the <code>label()</code> method on any base model:</p> <pre><code>base_model.label_folder(\n    input_folder=\"./images\",\n    output_folder=\"./labeled-images\",\n    sahi=True\n)\n</code></pre>"},{"location":"utilities/sahi/#see-also","title":"See Also","text":"<ul> <li>Using SAHI with supervision</li> </ul>"},{"location":"utilities/use-embeddings-in-classification/","title":"Use Embeddings in Classification","text":"<p>You can use embeddings in an <code>EmbeddingOntology</code> to classify images and detections with Autodistill.</p> <p>This has two uses:</p> <ol> <li>Classify entire images using embeddings computed with an embedding model, and;</li> <li>Classify regions of an image using the ComposedDetectionModel API.</li> </ol> <p>This API is especially useful if a classification model with embedding support (i.e. CLIP) struggles with a text prompt you provide.</p> <p>Consider a scenario where you want to classify vinyl records. You could compute an embedding for each album cover, then use those embeddings for classification.</p> <p>There are two <code>EmbeddingOntology</code> classes:</p> <ul> <li><code>EmbeddingOntologyImage</code>: Accepts a mapping from a text prompt (the class you will use in labeling) to an embedding. The embedding model you use for labeling (i.e. CLIP) will automatically compute embeddings for each image.</li> <li><code>EmbeddingOntologyRaw</code>: Accepts a mapping from a text prompt (the class you will use in labeling) to an embedding. You must compute the embeddings yourself, then provide them to the <code>EmbeddingOntologyRaw</code> class.</li> </ul> <p>In most cases, <code>EmbeddingOntologyImage</code> is the best choice, because Autodistill handles loading the model.</p> <p>However, if you already have embeddings, <code>EmbeddingOntologyRaw</code> is a better choice.</p> <p>If you use <code>EmbeddingOntologyImage</code> with pre-computed embeddings, you must use the same embedding model as the model you use for classification in Autodistill, otherwise auto-labeling will return inaccurate results.</p>"},{"location":"utilities/use-embeddings-in-classification/#embeddingontologyimage-example","title":"EmbeddingOntologyImage Example","text":"<p>In the example below, Grounding DINO is used to detect album covers, then CLIP is used to classify the album covers.</p> <p>Six images are provided as references in the <code>EmbeddingOntologyImage</code> class. These images are embedded by CLIP, then used for classification for each vinyl record detected by Grounding DINO.</p> <p>Learn more about the ComposedDetectionModel API.</p> <pre><code>from autodistill_clip import CLIP\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_grounding_dino import GroundingDINO\nfrom autodistill.core import EmbeddingOntologyImage\nfrom autodistill.core.combined_detection_model import CombinedDetectionModel\n\nimport torch\nimport clip\nfrom PIL import Image\nimport os\nimport cv2\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nINPUT_FOLDER = \"samples\"\nDATASET_INPUT = \"./images\"\nDATASET_OUTPUT = \"./dataset\"\nPROMPT = \"album cover\"\n\nimages = os.listdir(\"samples\")\n\nimages_to_classes = {\n    \"midnights\": \"IMG_9022.jpeg\",\n    \"men amongst mountains\": \"323601467684.jpeg\",\n    \"we are\": \"IMG_9056.jpeg\",\n    \"oh wonder\": \"Images (5).jpeg\",\n    \"brightside\": \"Images (4).jpeg\",\n    \"tears for fears\": \"Images (3).jpeg\"\n}\n\nmodel = CombinedDetectionModel(\n    detection_model=GroundingDINO(\n        CaptionOntology({PROMPT: PROMPT})\n    ),\n    classification_model=CLIP(\n        EmbeddingOntologyImage(images_to_classes)\n    )\n)\n\nresult = model.predict(\"./images/example.jpeg\")\n\nplot(\n    image=cv2.imread(\"./images/example.jpeg\"),\n    detections=result\n)\n</code></pre> <p>Here is the result from inference:</p> <p></p> <p>The album cover is annotated with the label \"men amougst mountains\".</p> <p>Grounding DINO successfully identified an album cover, then our EmbeddingOntologyImage classified the album cover.</p>"},{"location":"utilities/use-embeddings-in-classification/#embeddingontologyraw-example","title":"EmbeddingOntologyRaw Example","text":"<p>In the example below, we load embeddings from a file, where embeddings are in the form <code>{prompt: embedding}</code>.</p> <pre><code>from autodistill_clip import CLIP\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_grounding_dino import GroundingDINO\nfrom autodistill.core import EmbeddingOntology\nfrom autodistill.core.custom_detection_model import CustomDetectionModel\n\nimport torch\nimport clip\nfrom PIL import Image\nimport os\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nINPUT_FOLDER = \"samples\"\nDATASET_INPUT = \"./images\"\nDATASET_OUTPUT = \"./dataset\"\nPROMPT = \"album cover\"\n\nimages = os.listdir(\"samples\")\n\nwith open(\"embeddings.json\", \"r\") as f:\n    classes_to_embeddings = json.load(f)\n\nSAMCLIP = CustomDetectionModel(\n    detection_model=GroundingDINO(\n        CaptionOntology({PROMPT: PROMPT})\n    ),\n    classification_model=CLIP(\n        EmbeddingOntology(classes_to_embeddings.items())\n    )\n)\n\nresult = model.predict(\"./images/example.jpeg\")\n\nplot(\n    image=cv2.imread(\"./images/example.jpeg\"),\n    detections=result\n)\n</code></pre>"},{"location":"utilities/use-embeddings-in-classification/#code-reference","title":"Code Reference","text":""},{"location":"utilities/use-embeddings-in-classification/#autodistill.core.embedding_ontology.compare_embeddings","title":"<code>compare_embeddings(image_embedding, comparison_embeddings, distance_metric='cosine')</code>","text":"<p>Calculate the similarity between an image embedding and all embeddings in a list.</p> <p>Parameters:</p> Name Type Description Default <code>image_embedding</code> <code>np.ndarray</code> <p>The embedding of the image to compare.</p> required <code>comparison_embeddings</code> <code>List[np.ndarray]</code> <p>A list of embeddings to compare against.</p> required <code>distance_metric</code> <p>The distance metric to use. Currently only supports \"cosine\".</p> <code>'cosine'</code> <p>Returns:</p> Type Description <p>A list of similarity scores.</p> Source code in <code>autodistill/core/embedding_ontology.py</code> <pre><code>def compare_embeddings(\n    image_embedding: np.ndarray,\n    comparison_embeddings: List[np.ndarray],\n    distance_metric=\"cosine\",\n):\n\"\"\"\n    Calculate the similarity between an image embedding and all embeddings in a list.\n\n    Args:\n        image_embedding: The embedding of the image to compare.\n        comparison_embeddings: A list of embeddings to compare against.\n        distance_metric: The distance metric to use. Currently only supports \"cosine\".\n\n    Returns:\n        A list of similarity scores.\n    \"\"\"\n    if distance_metric == \"cosine\":\n        comparisons = []\n\n        for comparison_embedding in comparison_embeddings:\n            comparisons.append(\n                cosine_similarity(\n                    image_embedding.reshape(1, -1), comparison_embedding.reshape(1, -1)\n                ).flatten()\n            )\n\n        return sv.Classifications(\n            class_id=np.array([i for i in range(len(comparisons))]),\n            confidence=np.array(comparisons).flatten(),\n        )\n    else:\n        raise NotImplementedError(\n            f\"Distance metric {distance_metric} is not supported.\"\n        )\n</code></pre>"},{"location":"utilities/visualize-predictions/","title":"Visualize Predictions","text":"<p>The <code>plot()</code> method allows you to visualize predictions from a detection or segmentation model.</p> <p>If you use a detection model to run inference (i.e. Grounding DINO), the <code>plot()</code> method will plot bounding boxes for each prediction.</p> <p>If you use a segmentation model to run inference (i.e. Grounded SAM), the <code>plot()</code> method will plot segmentation masks for each prediction. </p> <p>Here is an example of the method used to annotate predictions from a Grounding DINO model:</p> Bounding BoxSegmentation Mask <pre><code>from autodistill_grounding_dino import GroundingDINO\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils.plot import plot\nimport cv2\n\nontology = CaptionOntology(\n    {\n        \"dog\": \"dog\",\n    }\n)\n\nmodel = GroundingDINO(ontology=ontology)\n\nresult = model.predict(\"./dog.jpeg\")\n\nplot(\n    image=cv2.imread(\"./dog.jpeg\"),\n    classes=base_model.ontology.classes(),\n    detections=result\n)\n</code></pre> <p></p> <pre><code>from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\nimport cv2\n\nontology = CaptionOntology(\n    {\n        \"dog\": \"dog\",\n    }\n)\n\nmodel = GroundedSAM(ontology=ontology)\n\nresult = model.predict(\"./dog.jpeg\")\n\nplot(\n    image=cv2.imread(\"./dog.jpeg\"),\n    classes=model.ontology.classes(),\n    detections=result\n)\n</code></pre> <p></p>"},{"location":"utilities/visualize-predictions/#code-reference","title":"Code Reference","text":"<p>Plot bounding boxes or segmentation masks on an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>The image to plot on</p> required <code>detections</code> <p>The detections to plot</p> required <code>classes</code> <code>List[str]</code> <p>The classes to plot</p> required <code>raw</code> <p>Whether to return the raw image or plot it interactively</p> <code>False</code> <p>Returns:</p> Type Description <p>The raw image (np.ndarray) if raw=True, otherwise None (image is plotted interactively</p> Source code in <code>autodistill/utils.py</code> <pre><code>def plot(image: np.ndarray, detections, classes: List[str], raw=False):\n\"\"\"\n    Plot bounding boxes or segmentation masks on an image.\n\n    Args:\n        image: The image to plot on\n        detections: The detections to plot\n        classes: The classes to plot\n        raw: Whether to return the raw image or plot it interactively\n\n    Returns:\n        The raw image (np.ndarray) if raw=True, otherwise None (image is plotted interactively\n    \"\"\"\n    # TODO: When we have a classification annotator\n    # in supervision, we can add it here\n    if detections.mask is not None:\n        annotator = sv.MaskAnnotator()\n    else:\n        annotator = sv.BoxAnnotator()\n\n    label_annotator = sv.LabelAnnotator()\n\n    labels = [\n        f\"{classes[class_id]} {confidence:0.2f}\"\n        for _, _, confidence, class_id, _, _ in detections\n    ]\n\n    annotated_frame = annotator.annotate(scene=image.copy(), detections=detections)\n    annotated_frame = label_annotator.annotate(\n        scene=annotated_frame, labels=labels, detections=detections\n    )\n\n    if raw:\n        return annotated_frame\n\n    sv.plot_image(annotated_frame, size=(8, 8))\n</code></pre>"}]}